{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8847004,"sourceType":"datasetVersion","datasetId":5324942}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:50:38.790833Z","iopub.execute_input":"2024-07-05T09:50:38.791284Z","iopub.status.idle":"2024-07-05T09:50:55.760525Z","shell.execute_reply.started":"2024-07-05T09:50:38.791249Z","shell.execute_reply":"2024-07-05T09:50:55.759230Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:51:03.998295Z","iopub.execute_input":"2024-07-05T09:51:03.998690Z","iopub.status.idle":"2024-07-05T09:51:24.988173Z","shell.execute_reply.started":"2024-07-05T09:51:03.998655Z","shell.execute_reply":"2024-07-05T09:51:24.987218Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-05 09:51:14.140838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-05 09:51:14.141006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-05 09:51:14.280892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load and preprocess the dataset\ndf = pd.read_csv(\"/kaggle/input/ticket/data.csv\")\ndf = df.drop(columns=['Unnamed: 0'])\ndf['sentence'] = df['clean_subject'] + ' ' + df['clean_description']\ndf = df[df['ticket_type'] != 'Problem']  # Drop Class Problem\ndf.dropna(subset=['sentence'], inplace=True)\ndf = df[['sentence', 'ticket_type']]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:51:24.990061Z","iopub.execute_input":"2024-07-05T09:51:24.990742Z","iopub.status.idle":"2024-07-05T09:52:01.674483Z","shell.execute_reply.started":"2024-07-05T09:51:24.990708Z","shell.execute_reply":"2024-07-05T09:52:01.673480Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Encode the labels\nlabel2id = {label: i for i, label in enumerate(df['ticket_type'].unique())}\nid2label = {i: label for label, i in label2id.items()}\ndf['label'] = df['ticket_type'].map(label2id)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:52:01.675875Z","iopub.execute_input":"2024-07-05T09:52:01.676306Z","iopub.status.idle":"2024-07-05T09:52:01.730168Z","shell.execute_reply.started":"2024-07-05T09:52:01.676268Z","shell.execute_reply":"2024-07-05T09:52:01.728941Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# Convert pandas DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Combine into a DatasetDict\ndatasets = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:53:39.127497Z","iopub.execute_input":"2024-07-05T09:53:39.128456Z","iopub.status.idle":"2024-07-05T09:53:41.350996Z","shell.execute_reply.started":"2024-07-05T09:53:39.128422Z","shell.execute_reply":"2024-07-05T09:53:41.350034Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load the model and tokenizer\nmodel_name = \"papluca/xlm-roberta-base-language-detection\"\nnum_labels = len(label2id)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = datasets.map(tokenize_function, batched=True)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:57:34.123773Z","iopub.execute_input":"2024-07-05T09:57:34.124489Z","iopub.status.idle":"2024-07-05T09:59:33.756157Z","shell.execute_reply.started":"2024-07-05T09:57:34.124453Z","shell.execute_reply":"2024-07-05T09:59:33.755074Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection and are newly initialized because the shapes did not match:\n- classifier.out_proj.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.out_proj.weight: found shape torch.Size([20, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/176680 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19438f3d0e4f4827980ada364c13cda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/44171 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca093e529d745b48b64471be68a2033"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:59:43.904090Z","iopub.execute_input":"2024-07-05T09:59:43.905030Z","iopub.status.idle":"2024-07-05T09:59:44.296871Z","shell.execute_reply.started":"2024-07-05T09:59:43.904994Z","shell.execute_reply":"2024-07-05T09:59:44.295605Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"03e121b5249ddc41bae22e9ba18fbb65bed41b1e","metadata":{}},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:59:45.931400Z","iopub.execute_input":"2024-07-05T09:59:45.932343Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240705_100025-jy806rno</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface/runs/jy806rno' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface' target=\"_blank\">https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface/runs/jy806rno' target=\"_blank\">https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface/runs/jy806rno</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1393' max='22086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1393/22086 20:50 < 5:10:08, 1.11 it/s, Epoch 0.13/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model\nmodel.save_pretrained(\"./xlm-model\")\ntokenizer.save_pretrained(\"./xlm-model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **model performance**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\n# Function to compute metrics\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n    acc = accuracy_score(p.label_ids, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\n# Evaluate the model\nresults = trainer.evaluate()\n\nprint(\"Evaluation results:\")\nfor key, value in results.items():\n    print(f\"{key}: {value:.4f}\")\n\n# Compute detailed metrics\ntest_preds = trainer.predict(tokenized_datasets[\"test\"])\nmetrics = compute_metrics(test_preds)\n\nprint(\"\\nDetailed metrics:\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **model interpretability**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom captum.attr import IntegratedGradients\nimport torch\n\n# Load the fine-tuned model and tokenizer\nmodel_path = \"./xlm-model\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Create a pipeline for text classification\nnlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\n# Function to interpret model predictions\ndef interpret_text(text):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    def forward_pass(inputs):\n        outputs = model(**inputs)\n        return torch.softmax(outputs.logits, dim=1)\n\n    integrated_gradients = IntegratedGradients(forward_pass)\n    attributions, delta = integrated_gradients.attribute(inputs['input_ids'], return_convergence_delta=True)\n    attributions_sum = attributions.sum(dim=2).squeeze(0)\n    \n    # Tokenize input text\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n    \n    # Get prediction\n    prediction = nlp(text)\n    \n    return tokens, attributions_sum, prediction\n\n# Example usage\nexample_text = \"Your example sentence here\"\ntokens, attributions, prediction = interpret_text(example_text)\n\nprint(f\"Text: {example_text}\")\nprint(f\"Prediction: {prediction}\")\nprint(\"\\nToken attributions:\")\nfor token, attribution in zip(tokens, attributions):\n    print(f\"{token}: {attribution.item():.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}