{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8847004,"sourceType":"datasetVersion","datasetId":5324942}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:50:38.790833Z","iopub.execute_input":"2024-07-05T09:50:38.791284Z","iopub.status.idle":"2024-07-05T09:50:55.760525Z","shell.execute_reply.started":"2024-07-05T09:50:38.791249Z","shell.execute_reply":"2024-07-05T09:50:55.759230Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:51:03.998295Z","iopub.execute_input":"2024-07-05T09:51:03.998690Z","iopub.status.idle":"2024-07-05T09:51:24.988173Z","shell.execute_reply.started":"2024-07-05T09:51:03.998655Z","shell.execute_reply":"2024-07-05T09:51:24.987218Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-05 09:51:14.140838: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-05 09:51:14.141006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-05 09:51:14.280892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load and preprocess the dataset\ndf = pd.read_csv(\"/kaggle/input/ticket/data.csv\")\ndf = df.drop(columns=['Unnamed: 0'])\ndf['sentence'] = df['clean_subject'] + ' ' + df['clean_description']\ndf = df[df['ticket_type'] != 'Problem']  # Drop Class Problem\ndf.dropna(subset=['sentence'], inplace=True)\ndf = df[['sentence', 'ticket_type']]","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:51:24.990061Z","iopub.execute_input":"2024-07-05T09:51:24.990742Z","iopub.status.idle":"2024-07-05T09:52:01.674483Z","shell.execute_reply.started":"2024-07-05T09:51:24.990708Z","shell.execute_reply":"2024-07-05T09:52:01.673480Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Encode the labels\nlabel2id = {label: i for i, label in enumerate(df['ticket_type'].unique())}\nid2label = {i: label for label, i in label2id.items()}\ndf['label'] = df['ticket_type'].map(label2id)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:52:01.675875Z","iopub.execute_input":"2024-07-05T09:52:01.676306Z","iopub.status.idle":"2024-07-05T09:52:01.730168Z","shell.execute_reply.started":"2024-07-05T09:52:01.676268Z","shell.execute_reply":"2024-07-05T09:52:01.728941Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# Convert pandas DataFrames to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Combine into a DatasetDict\ndatasets = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:53:39.127497Z","iopub.execute_input":"2024-07-05T09:53:39.128456Z","iopub.status.idle":"2024-07-05T09:53:41.350996Z","shell.execute_reply.started":"2024-07-05T09:53:39.128422Z","shell.execute_reply":"2024-07-05T09:53:41.350034Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load the model and tokenizer\nmodel_name = \"papluca/xlm-roberta-base-language-detection\"\nnum_labels = len(label2id)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, ignore_mismatched_sizes=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = datasets.map(tokenize_function, batched=True)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:57:34.123773Z","iopub.execute_input":"2024-07-05T09:57:34.124489Z","iopub.status.idle":"2024-07-05T09:59:33.756157Z","shell.execute_reply.started":"2024-07-05T09:57:34.124453Z","shell.execute_reply":"2024-07-05T09:59:33.755074Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection and are newly initialized because the shapes did not match:\n- classifier.out_proj.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.out_proj.weight: found shape torch.Size([20, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/176680 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19438f3d0e4f4827980ada364c13cda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/44171 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca093e529d745b48b64471be68a2033"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:59:43.904090Z","iopub.execute_input":"2024-07-05T09:59:43.905030Z","iopub.status.idle":"2024-07-05T09:59:44.296871Z","shell.execute_reply.started":"2024-07-05T09:59:43.904994Z","shell.execute_reply":"2024-07-05T09:59:44.295605Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"03e121b5249ddc41bae22e9ba18fbb65bed41b1e","metadata":{}},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T09:59:45.931400Z","iopub.execute_input":"2024-07-05T09:59:45.932343Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240705_100025-jy806rno</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface/runs/jy806rno' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface' target=\"_blank\">https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface/runs/jy806rno' target=\"_blank\">https://wandb.ai/narakorn-v-KHON%20KAEN%20UNIVERSITY/huggingface/runs/jy806rno</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1393' max='22086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1393/22086 20:50 < 5:10:08, 1.11 it/s, Epoch 0.13/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model\nmodel.save_pretrained(\"./xlm-model\")\ntokenizer.save_pretrained(\"./xlm-model\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **model performance**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\n# Function to compute metrics\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n    acc = accuracy_score(p.label_ids, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\n# Evaluate the model\nresults = trainer.evaluate()\n\nprint(\"Evaluation results:\")\nfor key, value in results.items():\n    print(f\"{key}: {value:.4f}\")\n\n# Compute detailed metrics\ntest_preds = trainer.predict(tokenized_datasets[\"test\"])\nmetrics = compute_metrics(test_preds)\n\nprint(\"\\nDetailed metrics:\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **model interpretability**","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom captum.attr import IntegratedGradients\nimport torch\n\n# Load the fine-tuned model and tokenizer\nmodel_path = \"./xlm-model\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Create a pipeline for text classification\nnlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n\n# Function to interpret model predictions\ndef interpret_text(text):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    def forward_pass(inputs):\n        outputs = model(**inputs)\n        return torch.softmax(outputs.logits, dim=1)\n\n    integrated_gradients = IntegratedGradients(forward_pass)\n    attributions, delta = integrated_gradients.attribute(inputs['input_ids'], return_convergence_delta=True)\n    attributions_sum = attributions.sum(dim=2).squeeze(0)\n    \n    # Tokenize input text\n    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n    \n    # Get prediction\n    prediction = nlp(text)\n    \n    return tokens, attributions_sum, prediction\n\n# Example usage\nexample_text = \"Your example sentence here\"\ntokens, attributions, prediction = interpret_text(example_text)\n\nprint(f\"Text: {example_text}\")\nprint(f\"Prediction: {prediction}\")\nprint(\"\\nToken attributions:\")\nfor token, attribution in zip(tokens, attributions):\n    print(f\"{token}: {attribution.item():.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}