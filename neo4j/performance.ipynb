{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/neo4j/performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUqSEnVta_yf",
        "outputId": "abeef189-e31d-4a70-aee9-a42d6967d481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m889.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install huggingface_hub transformers langchain langchain-community neo4j requests gradio torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Z4aTxVQSbGUb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import re\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key\n",
        "api_url02 = 'https://ai-api.manageai.co.th/llm-model-02/'\n",
        "api_url03 = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'\n"
      ],
      "metadata": {
        "id": "csj0SYxZmsWS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**run this once**"
      ],
      "metadata": {
        "id": "p0W8G_D1eB79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "# def transform_dates_and_floats(tx):\n",
        "#     tx.run(\"\"\"\n",
        "#     MATCH (s:SaleOrder)\n",
        "#     WHERE s.ContractStartDate IS NOT NULL AND s.ContractEndDate IS NOT NULL\n",
        "#     WITH s,\n",
        "#          [item IN split(s.ContractStartDate, \"/\") | toInteger(item)] AS startComponents,\n",
        "#          [item IN split(s.ContractEndDate, \"/\") | toInteger(item)] AS endComponents\n",
        "#     WITH s, startComponents, endComponents,\n",
        "#          startComponents[1] AS startMonth,\n",
        "#          endComponents[1] AS endMonth\n",
        "#     SET s.ContractStartDate =\n",
        "#         CASE\n",
        "#             WHEN startMonth > 12 THEN\n",
        "#                 date({\n",
        "#                     day: startComponents[1],\n",
        "#                     month: startComponents[0],\n",
        "#                     year: startComponents[2]\n",
        "#                 })\n",
        "#             ELSE\n",
        "#                 date({\n",
        "#                     day: startComponents[0],\n",
        "#                     month: startMonth,\n",
        "#                     year: startComponents[2]\n",
        "#                 })\n",
        "#         END,\n",
        "#         s.ContractEndDate =\n",
        "#         CASE\n",
        "#             WHEN endMonth > 12 THEN\n",
        "#                 date({\n",
        "#                     day: endComponents[1],\n",
        "#                     month: endComponents[0],\n",
        "#                     year: endComponents[2]\n",
        "#                 })\n",
        "#             ELSE\n",
        "#                 date({\n",
        "#                     day: endComponents[0],\n",
        "#                     month: endMonth,\n",
        "#                     year: endComponents[2]\n",
        "#                 })\n",
        "#         END;\n",
        "#     \"\"\")\n",
        "\n",
        "# # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "# with driver.session() as session:\n",
        "#     session.write_transaction(transform_dates_and_floats)"
      ],
      "metadata": {
        "id": "ZqprXia6m4Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to calculate and set DaysDuration on SaleOrder nodes\n",
        "# def update_sale_order_duration(tx):\n",
        "#     tx.run(\"\"\"\n",
        "#     MATCH (so:SaleOrder)\n",
        "#     WHERE so.ContractStartDate IS NOT NULL AND so.ContractEndDate IS NOT NULL\n",
        "#     WITH so, duration.between(so.ContractStartDate, so.ContractEndDate) AS contractDuration\n",
        "#     SET so.DaysDuration = contractDuration.days + (contractDuration.months * 30) + (contractDuration.years * 365)\n",
        "#     RETURN so.SONumber, so.DaysDuration;\n",
        "#     \"\"\")\n",
        "\n",
        "# # Function to update duration for all SaleOrder nodes\n",
        "# def calculate_and_update_duration():\n",
        "#     with driver.session() as session:\n",
        "#         session.write_transaction(update_sale_order_duration)\n",
        "\n",
        "# # Run the update function\n",
        "# calculate_and_update_duration()"
      ],
      "metadata": {
        "id": "Z7Yel9dvoC9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Process**"
      ],
      "metadata": {
        "id": "TnPFha86mhDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_prompt():\n",
        "    return '''\n",
        "    You are an expert Cypher query generator for a graph database with the following nodes and relationships:\n",
        "\n",
        "    - **Nodes:**\n",
        "      - `Seller`: Represents a seller with properties: `id` (string), `name` (string).\n",
        "      - `Customer`: Represents a customer with properties: `id` (string), `name` (string).\n",
        "      - `SaleOrder`: Represents a sales order with properties: `SONumber` (string), `ContractStartDate` (Date), `ContractEndDate` (Date), `Total` (float), `DaysDuration` (integer).\n",
        "      - `CostSheet`: Represents a cost sheet with properties: `CSNumber` (string), `Internal` (float), `External` (float).\n",
        "      - `Service`: Represents a service with properties: `Service` (string), `Original` (string).\n",
        "      - `Platform`: Represents a platform with the property: `Original` (string).\n",
        "\n",
        "    - **Relationships:**\n",
        "      - `HAS_COST_SHEET`: Connects `SaleOrder` to `CostSheet`.\n",
        "      - `PROVIDES_SERVICE`: Connects `SaleOrder` to `Service`.\n",
        "      - `SERVICE_COST`: Connects `CostSheet` to `Service` with properties: `Internal` (float), `External` (float), `Total` (float).\n",
        "      - `DEPLOYED_ON`: Connects `Service` to `Platform`.\n",
        "      - `PLACED_ORDER`: Connects `Customer` to `SaleOrder` with properties: `ContractStartDate` (Date), `ContractEndDate` (Date), `DaysDuration` (integer).\n",
        "      - `HANDLED_ORDER`: Connects `Seller` to `SaleOrder`.\n",
        "\n",
        "    **Important Instructions**:\n",
        "    - Your task is to generate a **single Cypher query** based on the question.\n",
        "    - **Provide only the Cypher query, nothing else**. Do not include explanations, comments, or any additional text.\n",
        "    - Ensure the query is **valid** and uses **correct property and relationship names**.\n",
        "    - Stop after generating the query (end with \";\").\n",
        "    - **Do not use SQL-style subqueries**\n",
        "    - Do not use SQL-style subqueries like 'SELECT MAX'. Instead, sort the results and limit it to get the highest or lowest value.\n",
        "    - To get the record with the maximum value, you must use `ORDER BY` within the main query.\n",
        "\n",
        "    Your task is to generate a **single Cypher query** based on the question.\n",
        "\n",
        "    - **Provide only the Cypher query, nothing else.**\n",
        "    - **Do not provide explanations, markdown syntax, or additional words except the query.**\n",
        "    - Ensure the query is **valid** and uses **correct property and relationship names**.\n",
        "    - Stop after generating the query (end with \";\").\n",
        "\n",
        "    Given the question: {question}\n",
        "    '''\n",
        "\n",
        "def get_answer_prompt():\n",
        "    return '''\n",
        "    Given the question: \"{question}\" and the answer record from the graph database: \"{result_record}\".\n",
        "    You are an expert chatbot designed to provide a confident, clear, concise and human-like response based on the question and query results from a graph database.\n",
        "\n",
        "    ### Important:\n",
        "    - Your response must ONLY contain the essential answer. Do not include any of the instructions, question text, or explanation.\n",
        "\n",
        "    ### Instructions (For the chatbot only, DO NOT include this in the response):\n",
        "    - Respond with a direct, single response that provides the correct and essential information, without repeating or justifying your answer.\n",
        "    - Answer with confidence and **only the essential information**, **without repeating any words or parts of the original question or prompt**.\n",
        "    - The response should be as the same language as the input question (English or Thai).\n",
        "    - Format the currency as \"Bath\" when discussing costs, profits, or discounts. (never copy this instruction).\n",
        "    - If there is **only one record**, respond with a **single, direct sentence**.\n",
        "    - If there are **multiple records**, use **bullet points** to clearly present each entry without repeating or summarizing.\n",
        "\n",
        "    ### WHAT TO DO (For the chatbot only, DO NOT include this in the response):\n",
        "    - Provide the answer with **only the core information** (e.g., customer name, order count) in a single, clear sentence.\n",
        "    - Ensure **no part of the original question or prompt text** appears in the response.\n",
        "    - Keep the response focused and concise, **without any repetition** or unnecessary commentary.\n",
        "    - If there are multiple entries, list each in a **bullet point format** for readability. Ensure that the response is formatted for clarity and precision.\n",
        "\n",
        "    ### WHAT NOT TO DO (For the chatbot only, DO NOT include this in the response):\n",
        "    - DO NOT repeat words or phrases from the original prompt or question.\n",
        "    - DO NOT provide any justifications, corrections, or revisions of the answer.\n",
        "    - DO NOT include any extra formatting, headers, comments, symbols, examples, or explanations.\n",
        "    - DO NOT mention \"assistant\" or phrases like \"based on the query\" or \"according to the data.\"\n",
        "    - DO NOT include any phrases expressing uncertainty, apologies, or attempts to correct previous responses.\n",
        "    '''"
      ],
      "metadata": {
        "id": "2Eb2eL_RbPtS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**regex**"
      ],
      "metadata": {
        "id": "2qDd0Eo1bIqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract(response):\n",
        "    try:\n",
        "        # Step 1: Remove specific unwanted patterns like {}, /* but keep the operation symbols (+, -, *, /) and the comma \",\"\n",
        "        # clean_response = re.sub(r'[\"{}/*]+', '', response)  # Remove \"{\", \"}\", but keep *, +, - and /\n",
        "\n",
        "        # Step 2: Remove code block markers and extra formatting\n",
        "        clean_response = re.sub(r'```cypher|```', '', response)\n",
        "        clean_response = re.sub(r'^\\*/\\s*', '', clean_response, flags=re.MULTILINE)\n",
        "        clean_response = clean_response.replace(\"### Response:\", \"\").strip()\n",
        "        clean_response = re.sub(r'\\bassistant\\b.*$', '', clean_response, flags=re.DOTALL | re.IGNORECASE).strip()\n",
        "        # Step 2: Remove everything before and including \"Response:\" (case-insensitive)\n",
        "        response_split = re.split(r'response:', clean_response, flags=re.IGNORECASE)\n",
        "        if len(response_split) > 1:\n",
        "            clean_response = response_split[1].strip()  # Keep only the part after \"Response:\"\n",
        "\n",
        "        # Step 5: Find the index of the word \"MATCH\" (case-insensitive) and slice the string from that point onwards\n",
        "        match_index = clean_response.lower().find(\"match\")\n",
        "        if match_index != -1:\n",
        "            clean_response = clean_response[match_index:].strip()  # Keep everything from \"MATCH\" onwards\n",
        "\n",
        "        # Step 6: Try to find and return the first semicolon (for Cypher queries)\n",
        "        match = re.search(r'([^;]*;)', clean_response)\n",
        "        if match:\n",
        "            return match.group(0)  # Return the matched query including the semicolon\n",
        "\n",
        "        # Step 7: If no semicolon, split the cleaned response to extract content\n",
        "        cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "        extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "        if extracted_queries:\n",
        "            seen_queries = set()\n",
        "            unique_queries = []\n",
        "            for query in extracted_queries:\n",
        "                if query not in seen_queries:\n",
        "                    seen_queries.add(query)\n",
        "                    unique_queries.append(query)\n",
        "            return unique_queries[0] if unique_queries else None\n",
        "        else:\n",
        "            return clean_response.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during extraction: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "4E8CBjYAbK0m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cypher_query(cypher_query):\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            # Execute the query\n",
        "            result = session.run(cypher_query)\n",
        "            return result.data()  # Return all records as a list of dictionaries\n",
        "    except Exception as e:\n",
        "        return f\"Error running Cypher query: {str(e)}\"\n",
        "\n",
        "def format_result_record(result_record, max_records=10):\n",
        "    if isinstance(result_record, list):\n",
        "        limited_records = result_record[:max_records]\n",
        "        formatted_strings = [\", \".join([f\"{key}: {value}\" for key, value in record.items()]) for record in limited_records]\n",
        "        return \"\\n\".join(formatted_strings)\n",
        "    return str(result_record)\n",
        "\n",
        "context_data = {\n",
        "    \"conversations\": []  # Store a list of interactions\n",
        "}\n",
        "\n",
        "def update_context_data(context_data, question, cypher_query, result_record, answer):\n",
        "    # Store each interaction as a dictionary\n",
        "    interaction = {\n",
        "        \"question\": question,\n",
        "        \"cypher_query\": cypher_query,\n",
        "        \"result_record\": result_record,\n",
        "        \"answer\": answer\n",
        "    }\n",
        "    # Append this interaction to the conversation history\n",
        "    context_data[\"conversations\"].append(interaction)"
      ],
      "metadata": {
        "id": "P_95XOkIbQSS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **query prompt**"
      ],
      "metadata": {
        "id": "FD7V4cKykwJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt1(question, context_data=None):\n",
        "    try:\n",
        "        baseprompt = get_base_prompt()\n",
        "\n",
        "        # Add conversation history context to the prompt\n",
        "        if context_data and \"conversations\" in context_data:\n",
        "            conversation_history = \"\"\n",
        "            for interaction in context_data[\"conversations\"]:\n",
        "                # Add each conversation's question, cypher query, result, and answer to the history\n",
        "                conversation_history += f\"Previous Question: {interaction['question']}\\n\"\n",
        "                conversation_history += f\"Cypher Query: {interaction['cypher_query']}\\n\"\n",
        "                conversation_history += f\"Result: {interaction['result_record']}\\n\"\n",
        "                conversation_history += f\"Answer: {interaction['answer']}\\n\\n\"\n",
        "\n",
        "            # Append this context to the current question\n",
        "            formatted_prompt = baseprompt.replace(\"{question}\", question + \"\\n\\n\" + conversation_history)\n",
        "        else:\n",
        "            # If no context, just use the current question\n",
        "            formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "\n",
        "        # Model parameters\n",
        "        model_params = {\n",
        "            'max_new_tokens': 512,\n",
        "            'temperature': 0.001,\n",
        "            'top_p': 0.95,\n",
        "            'repetition_penalty': 1.0\n",
        "        }\n",
        "\n",
        "        # Send the prompt to the language model for Cypher generation\n",
        "        client = InferenceClient(api_url03, api_key)\n",
        "        response = client.text_generation(formatted_prompt, **model_params)\n",
        "\n",
        "        # Extract the Cypher query from the LLM response\n",
        "        clean_cypher_query = extract(response.strip())\n",
        "        return clean_cypher_query\n",
        "    except Exception as e:\n",
        "        return f\"Error generating query: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "tPE4OkiUkc0G"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **answer prompt**"
      ],
      "metadata": {
        "id": "1AbtX7cFkz8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def prompt2(question, result_record, context_data=None):\n",
        "    try:\n",
        "        answer_prompt = get_answer_prompt()\n",
        "        formatted_result = format_result_record(result_record)\n",
        "\n",
        "        # Include the full conversation history in the prompt\n",
        "        if context_data and \"conversations\" in context_data:\n",
        "            conversation_history = \"\"\n",
        "            for interaction in context_data[\"conversations\"]:\n",
        "                conversation_history += f\"Previous Question: {interaction['question']}\\n\"\n",
        "                conversation_history += f\"Answer: {interaction['answer']}\\n\\n\"\n",
        "\n",
        "            # Add the conversation history into the prompt\n",
        "            formatted_prompt = (answer_prompt.replace(\"{question}\", question).replace(\"{result_record}\", formatted_result)\n",
        "                               + \"\\n\\n\" + conversation_history)\n",
        "        else:\n",
        "            # Use only the current question and result if no history\n",
        "            formatted_prompt = (answer_prompt.replace(\"{question}\", question).replace(\"{result_record}\", formatted_result))\n",
        "\n",
        "        # Parameters for LLM generation\n",
        "        model_params = {\n",
        "            'max_new_tokens': 512,\n",
        "            'temperature': 0.001,\n",
        "            'top_p': 0.99,\n",
        "            'repetition_penalty': 1.0\n",
        "        }\n",
        "\n",
        "        client = InferenceClient(api_url03, api_key)\n",
        "        response = client.text_generation(formatted_prompt, **model_params)\n",
        "        clean_response = extract(response.strip())\n",
        "\n",
        "        # Ensure the response is well formatted\n",
        "        if clean_response:\n",
        "            return clean_response.strip()\n",
        "        else:\n",
        "            return f\"No answer generated. Results:\\n{formatted_result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "gXI_hG4-byfv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def chatbot_response(message, chat_history, context_data=None):\n",
        "    try:\n",
        "        # Get the Cypher query while passing the full context_data\n",
        "        cypher_query = prompt1(message, context_data)\n",
        "        print(f\"Generated Cypher Query: {cypher_query}\")\n",
        "\n",
        "        if cypher_query:\n",
        "            result_record = run_cypher_query(cypher_query)\n",
        "            print(f\"Result Record: {result_record}\")\n",
        "\n",
        "            if result_record and isinstance(result_record, list):\n",
        "                # Generate the answer using the full context\n",
        "                answer = await prompt2(message, result_record, context_data)\n",
        "\n",
        "                # Append the interaction to the conversation history\n",
        "                update_context_data(context_data, message, cypher_query, result_record, answer)\n",
        "\n",
        "                chat_history.append((message, answer))  # Update the chat history with the new interaction\n",
        "            else:\n",
        "                chat_history.append((message, \"No relevant data found in the database.\"))\n",
        "                return \"\", chat_history, context_data  # Return here with empty answer\n",
        "        else:\n",
        "            chat_history.append((message, \"Failed to generate a valid Cypher query.\"))\n",
        "            return \"\", chat_history, context_data  # Return here with empty answer\n",
        "    except Exception as e:\n",
        "        chat_history.append((message, f\"Error: {str(e)}\"))\n",
        "        return \"\", chat_history, context_data  # Ensure three values are returned\n",
        "\n",
        "    return \"\", chat_history, context_data  # Final return statement\n"
      ],
      "metadata": {
        "id": "nNevhDl9kngJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## interface"
      ],
      "metadata": {
        "id": "tcF_6Eer8ZIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Gradio theme and layout\n",
        "theme = gr.themes.Default(\n",
        "    text_size=\"sm\",\n",
        "    primary_hue=\"blue\"\n",
        ")\n",
        "\n",
        "# Clear chat history and context_data\n",
        "def clear_chat():\n",
        "    global context_data  # Declare context_data as global to modify it\n",
        "    context_data[\"conversations\"] = []  # Clear conversation history\n",
        "    return None, []  # Clear chatbot UI and textbox\n",
        "\n",
        "with gr.Blocks(theme=theme) as demo:\n",
        "    gr.Markdown(\"<h1 style='text-align: center;'>Sales Assistant Chatbot üí¨</h1>\", elem_id=\"title\")\n",
        "\n",
        "    with gr.Column(elem_id=\"chatbot-container\"):\n",
        "        chatbot_ui = gr.Chatbot(label=\"Chatbot\", elem_id=\"gr-chatbot\")  # Chatbot UI\n",
        "        msg = gr.Textbox(placeholder=\"Ask about orders, services, and more...\", label=\"Type here\", show_label=False)\n",
        "        clear = gr.Button(\"Clear\", elem_id=\"gr-button\")\n",
        "\n",
        "        # Submit message and get response\n",
        "        async def submit_action(message, chat_history):\n",
        "            global context_data  # Use the global context_data variable\n",
        "\n",
        "            response, chat_history, context_data = await chatbot_response(message, chat_history, context_data)\n",
        "            return response, chat_history\n",
        "\n",
        "        # Set up the message submission to call submit_action\n",
        "        msg.submit(submit_action, [msg, chatbot_ui], [msg, chatbot_ui])\n",
        "\n",
        "        # Clear button functionality - Reset chat history and conversation context\n",
        "        clear.click(clear_chat, None, [msg, chatbot_ui])  # Reset chat UI and textbox, clear history\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "m0Eh7cxTnAh8",
        "outputId": "ef1bfb0b-3483-4b07-ac72-310ecd9c898b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f8cbb657c759de702b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f8cbb657c759de702b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in '__init__': pass token='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' as keyword args. From version 0.26 passing these as positional arguments will result in an error,\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Cypher Query: MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder) RETURN c.name, sum(so.Total) as total ORDER BY total DESC LIMIT 1;\n",
            "Result Record: [{'c.name': '‡∏ö‡∏±‡∏ï‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢', 'total': 5028824.74}]\n",
            "Generated Cypher Query: MATCH (c:Customer {name: '‡∏ö‡∏±‡∏ï‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢'})-[:PLACED_ORDER]->(so:SaleOrder) RETURN count(so) as count;\n",
            "Result Record: [{'count': 24}]\n",
            "Generated Cypher Query: MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder) RETURN c.name, sum(so.Total) as total ORDER BY total DESC LIMIT 1;\n",
            "Result Record: [{'c.name': '‡∏ö‡∏±‡∏ï‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢', 'total': 5028824.74}]\n",
            "Generated Cypher Query: MATCH (c:Customer {name: '‡∏ö‡∏±‡∏ï‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢'})-[:PLACED_ORDER]->(so:SaleOrder)-[:PROVIDES_SERVICE]->(s:Service) RETURN s.Service, COUNT(so) as count ORDER BY count DESC LIMIT 2;\n",
            "Result Record: [{'s.Service': 'Network', 'count': 11}, {'s.Service': 'IDC2', 'count': 10}]\n",
            "Generated Cypher Query: MATCH (c:Customer {name: '‡∏ö‡∏±‡∏ï‡∏£‡∏Å‡∏£‡∏∏‡∏á‡πÑ‡∏ó‡∏¢'})-[:PLACED_ORDER]->(so:SaleOrder) RETURN COUNT(so) as count;\n",
            "Result Record: [{'count': 24}]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f8cbb657c759de702b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **question**"
      ],
      "metadata": {
        "id": "fLEWX_vsNI05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Which customer has placed the highest total value of sales orders?\n",
        "# 2. Which sale order has the highest total value, and what services are linked to that sale order?\n",
        "# 3. find the sales order with the longest contract duration, including its linked services. Make sure to limit the result to one sales order for efficiency.\n",
        "# 4. Who is the sellers that handling the most valuable orders (based on total sales)?\n",
        "# 5. Top 5 services are deployed on the most platforms?\n",
        "# 6. the top 5 services that are most frequently associated with SaleOrder\n",
        "# 7. the top 5 services that are least frequently associated with SaleOrder\n",
        "# 8. Top 5 customers that have placed the most SaleOrders along with their number of sale orders and summation total value of SaleOrders\n",
        "# 9. Every SaleOrder has cost sheet, every costsheet has service cost, give me the top 5 services generate the highest internal costs for sellers?\n",
        "# 10. Can you provide the top 5 customers along with their sale orders and calculated discounts based on the DaysDuration from the SaleOrder? The discount should be calculated as 10% for every 365 days of the DaysDuration"
      ],
      "metadata": {
        "id": "tmtfjW1nNMKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on Key Details:\n",
        "# What is the total sales value for this customer?\n",
        "# How many orders has this customer placed?\n",
        "# What services has this customer used?\n",
        "# Can you list the last three orders placed by this customer?\n",
        "# What are the top two services this customer has used?"
      ],
      "metadata": {
        "id": "8hHDaQPrlviY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ id ‡πÅ‡∏•‡∏∞ ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏£‡∏ß‡∏°‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î?\n",
        "# 2. ‡∏ö‡∏≠‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ (SONumber) ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏î‡∏ö‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏ô‡∏±‡πâ‡∏ô?\n",
        "# 3. ‡∏ö‡∏≠‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ (SONumber) ‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏¢‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á\n",
        "# 4. ‡∏ú‡∏π‡πâ‡∏Ç‡∏≤‡∏¢‡∏Ñ‡∏ô‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏≠‡∏≠‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏Ç‡∏≤‡∏¢)?\n",
        "# 5. ‡∏ö‡∏≠‡∏Å 5 ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏û‡∏•‡∏ï‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î?\n",
        "# 6. ‡∏ö‡∏≠‡∏Å 5 ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á (PROVIDES SERVICE) ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ (SaleOrder) ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "# 7. ‡∏ö‡∏≠‡∏Å 5 ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á (PROVIDES SERVICE) ‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ (SaleOrder) ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "# 8. ‡∏ö‡∏≠‡∏Å 5 ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠\n",
        "# 9. ‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏µ cost sheet ‡∏ó‡∏∏‡∏Å cost sheet ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏¢‡∏ö‡∏≠‡∏Å‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ 5 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏¢‡πÉ‡∏ô  (Internal cost) ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡∏Ç‡∏≤‡∏¢?\n",
        "# 10. ‡∏ö‡∏≠‡∏Å 5 ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏±‡∏ç‡∏ç‡∏≤ - ‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏™‡∏±‡∏ç‡∏ç‡∏≤ (DaysDuration) ‡∏¢‡∏≤‡∏ß‡∏ô‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏ö‡∏≠‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î‡∏ï‡∏≤‡∏° DaysDuration ‡∏à‡∏≤‡∏Å SaleOrder ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°? ‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏õ‡πá‡∏ô 10% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÜ 365 ‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á DaysDuration"
      ],
      "metadata": {
        "id": "Cu-eQCRxWnHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Retain and Reference Chat Context**"
      ],
      "metadata": {
        "id": "A3h_d9f7cFgI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TU8VnoMkcLWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}