{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/neo4j/performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUqSEnVta_yf",
        "outputId": "2e832e66-da0a-4269-b57e-11e9db74e3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install huggingface_hub transformers langchain langchain-community neo4j requests gradio torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Z4aTxVQSbGUb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_prompt():\n",
        "    return '''\n",
        "    You are an expert Cypher query generator for a graph database with the following nodes and relationships:\n",
        "\n",
        "    - **Nodes:**\n",
        "      - `Seller`: Represents a seller with properties: `id` (string), `name` (string).\n",
        "      - `Customer`: Represents a customer with properties: `id` (string), `name` (string).\n",
        "      - `SaleOrder`: Represents a sales order with properties: `SONumber` (string), `ContractStartDate` (Date), `ContractEndDate` (Date), `Total` (float).\n",
        "      - `CostSheet`: Represents a cost sheet with properties: `CSNumber` (string), `Internal` (float), `External` (float).\n",
        "      - `Service`: Represents a service with properties: `Service` (string), `Original` (string).\n",
        "      - `Platform`: Represents a platform with the property: `Original` (string).\n",
        "\n",
        "    - **Relationships:**\n",
        "      - `HAS_COST_SHEET`: Connects `SaleOrder` to `CostSheet`.\n",
        "      - `PROVIDES_SERVICE`: Connects `SaleOrder` to `Service`.\n",
        "      - `SERVICE_COST`: Connects `CostSheet` to `Service` with properties: `Internal` (float), `External` (float).\n",
        "      - `DEPLOYED_ON`: Connects `Service` to `Platform`.\n",
        "      - `PLACED_ORDER`: Connects `Customer` to `SaleOrder` with properties: `ContractStartDate` (Date), `ContractEndDate` (Date).\n",
        "      - `HANDLED_ORDER`: Connects `Seller` to `SaleOrder`.\n",
        "\n",
        "    Your task is to generate a **single Cypher query** based on the question.\n",
        "\n",
        "    - Provide only the Cypher query, nothing else.\n",
        "    - Do not provide explanations, markdown syntax, or additional queries.\n",
        "    - Return the Cypher query **once**.\n",
        "    - Ensure the query is **valid** and uses **correct property and relationship names**.\n",
        "    - Stop after generating the query (end with \";\").\n",
        "\n",
        "    Given the question: {question}\n",
        "    '''\n",
        "\n",
        "\n",
        "def get_answer_prompt():\n",
        "    return '''\n",
        "    You are an expert designed to provide clear, concise answers based on query results from a graph database.\n",
        "\n",
        "    ### Instructions:\n",
        "    - Understand the question: \"{question}\".\n",
        "    - Review the result data: {result_record}.\n",
        "    - Respond with a brief, clear, and concise answer that directly addresses the question.\n",
        "    - Provide only the essential information, with no extra commentary, thinking process, or step descriptions.\n",
        "\n",
        "    ### Guidelines:\n",
        "    - The answer must be brief, directly addressing the question.\n",
        "    - Only include relevant information (e.g., customer ID and name), formatted cleanly.\n",
        "    - Avoid any additional commentary, repetition, or explanation of the thought process.\n",
        "    - Do not include titles or headers (e.g., \"Step 3\", \"Execute the function\").\n",
        "    - Ensure the output is in a clean sentence or bullet format with no repeated sentences, depending on the result.\n",
        "    '''"
      ],
      "metadata": {
        "id": "2Eb2eL_RbPtS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**convert date**"
      ],
      "metadata": {
        "id": "pGtfKDPpn1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ฟังก์ชันสำหรับรันคำสั่งแปลงข้อมูล\n",
        "# def transform_dates_and_floats(tx):\n",
        "#     tx.run(\"\"\"\n",
        "#     MATCH (s:SaleOrder)\n",
        "#     WHERE s.ContractStartDate IS NOT NULL AND s.ContractEndDate IS NOT NULL\n",
        "#     WITH s,\n",
        "#          [item IN split(s.ContractStartDate, \"/\") | toInteger(item)] AS startComponents,\n",
        "#          [item IN split(s.ContractEndDate, \"/\") | toInteger(item)] AS endComponents\n",
        "#     WITH s, startComponents, endComponents,\n",
        "#          startComponents[1] AS startMonth,\n",
        "#          endComponents[1] AS endMonth\n",
        "#     SET s.ContractStartDate =\n",
        "#         CASE\n",
        "#             WHEN startMonth > 12 THEN\n",
        "#                 date({\n",
        "#                     day: startComponents[1],\n",
        "#                     month: startComponents[0],\n",
        "#                     year: startComponents[2]\n",
        "#                 })\n",
        "#             ELSE\n",
        "#                 date({\n",
        "#                     day: startComponents[0],\n",
        "#                     month: startMonth,\n",
        "#                     year: startComponents[2]\n",
        "#                 })\n",
        "#         END,\n",
        "#         s.ContractEndDate =\n",
        "#         CASE\n",
        "#             WHEN endMonth > 12 THEN\n",
        "#                 date({\n",
        "#                     day: endComponents[1],\n",
        "#                     month: endComponents[0],\n",
        "#                     year: endComponents[2]\n",
        "#                 })\n",
        "#             ELSE\n",
        "#                 date({\n",
        "#                     day: endComponents[0],\n",
        "#                     month: endMonth,\n",
        "#                     year: endComponents[2]\n",
        "#                 })\n",
        "#         END;\n",
        "#     \"\"\")\n",
        "\n",
        "# # เรียกใช้งาน\n",
        "# with driver.session() as session:\n",
        "#     session.write_transaction(transform_dates_and_floats)"
      ],
      "metadata": {
        "id": "ZqprXia6m4Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **model 03**"
      ],
      "metadata": {
        "id": "3CufsxbwY0Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import asyncio\n",
        "import re\n",
        "from huggingface_hub import InferenceClient\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'\n",
        "\n",
        "# Exception handling during extraction\n",
        "def extract(response):\n",
        "    try:\n",
        "        # Remove code block markers\n",
        "        clean_response = re.sub(r'```cypher|```', '', response)\n",
        "        clean_response = re.sub(r'^\\*/\\s*', '', clean_response, flags=re.MULTILINE)\n",
        "\n",
        "        # Check for the word \"assistant\" and modify the response accordingly\n",
        "        if \"assistant\" in clean_response:\n",
        "            clean_response = clean_response.split(\"assistant\")[0].strip()\n",
        "\n",
        "        # Split the cleaned response to extract queries\n",
        "        cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "        extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "        if extracted_queries:\n",
        "            seen_queries = set()\n",
        "            unique_queries = []\n",
        "            for query in extracted_queries:\n",
        "                if query not in seen_queries:\n",
        "                    seen_queries.add(query)\n",
        "                    unique_queries.append(query)\n",
        "            return unique_queries[0] if unique_queries else None\n",
        "        else:\n",
        "            return clean_response.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error during extraction: {str(e)}\"\n",
        "\n",
        "def run_cypher_query(cypher_query):\n",
        "    try:\n",
        "        with driver.session() as session:\n",
        "            result = session.run(cypher_query)\n",
        "            return result.data()  # Return paginated records as a list of dictionaries\n",
        "    except Exception as e:\n",
        "        return f\"Error running Cypher query: {str(e)}\"\n",
        "\n",
        "# Function to format result records into a string\n",
        "def format_result_record(result_record):\n",
        "    if isinstance(result_record, list):\n",
        "        return \"\\n\".join([str(record) for record in result_record])\n",
        "    return str(result_record)\n",
        "\n",
        "# Synchronous call to LLM inference\n",
        "def prompt1(question):\n",
        "    try:\n",
        "        baseprompt = get_base_prompt()\n",
        "        formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "        model_params = {\n",
        "            'max_new_tokens': 512,\n",
        "            'temperature': 0.01,\n",
        "            'top_p': 0.95,\n",
        "            'repetition_penalty': 1.0\n",
        "        }\n",
        "\n",
        "        client = InferenceClient(api_url, api_key)\n",
        "        response = client.text_generation(formatted_prompt, **model_params)\n",
        "        clean_cypher_query = extract(response.strip())\n",
        "        return clean_cypher_query\n",
        "    except Exception as e:\n",
        "        return f\"Error generating query: {str(e)}\"\n",
        "\n",
        "async def prompt2(question, result_record):\n",
        "    try:\n",
        "        answer_prompt = get_answer_prompt()\n",
        "        formatted_result = format_result_record(result_record)\n",
        "        formatted_prompt = answer_prompt.replace(\"{question}\", question).replace(\"{result_record}\", formatted_result)\n",
        "\n",
        "        model_params = {\n",
        "            'max_new_tokens': 512,\n",
        "            'temperature': 0.001,\n",
        "            'top_p': 0.95,\n",
        "            'repetition_penalty': 1.0\n",
        "        }\n",
        "\n",
        "        client = InferenceClient(api_url, api_key)\n",
        "        response = client.text_generation(formatted_prompt, **model_params)\n",
        "        clean_response = extract(response.strip())\n",
        "        clean_response = clean_response.strip()\n",
        "\n",
        "        # Final check for empty responses\n",
        "        if clean_response == \"\":\n",
        "            return \"No data found.\"\n",
        "\n",
        "        return clean_response\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\"\n",
        "\n",
        "# Function to handle chatbot response\n",
        "async def chatbot_response(message, chat_history):\n",
        "    try:\n",
        "        cypher_query = prompt1(message)\n",
        "        print(f\"Generated Cypher Query: {cypher_query}\")  # Debugging print\n",
        "\n",
        "        if cypher_query:\n",
        "            result_record = run_cypher_query(cypher_query)\n",
        "            print(f\"Result Record: {result_record}\")  # Debugging print\n",
        "\n",
        "            if result_record and isinstance(result_record, list):\n",
        "                answer = await prompt2(message, result_record)  # Await the async function\n",
        "                chat_history.append((message, answer))\n",
        "            else:\n",
        "                chat_history.append((message, \"No relevant data found in the database.\"))\n",
        "        else:\n",
        "            chat_history.append((message, \"Failed to generate a valid Cypher query.\"))\n",
        "    except Exception as e:\n",
        "        chat_history.append((message, f\"Error: {str(e)}\"))\n",
        "\n",
        "    return \"\", chat_history\n",
        "\n",
        "# Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot_ui = gr.Chatbot(label=\"Chatbot\")\n",
        "    msg = gr.Textbox(placeholder=\"Ask a question about the cost sheet...\")\n",
        "    clear = gr.ClearButton([msg, chatbot_ui])\n",
        "\n",
        "    # Submit message and get response\n",
        "    msg.submit(chatbot_response, [msg, chatbot_ui], [msg, chatbot_ui])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "P_95XOkIbQSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}