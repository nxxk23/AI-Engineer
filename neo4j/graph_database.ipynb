{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjr7++SKPS1DQ/bpy+EVF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/neo4j/graph_database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install huggingface_hub transformers langchain langchain-community neo4j requests gradio torch"
      ],
      "metadata": {
        "id": "uMLiyiHgX0aq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 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\n",
        "# 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\n",
        "# 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\n",
        "# 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\n",
        "\n",
        "# NEO4J_URI=neo4j+s://19d8dc72.databases.neo4j.io\n",
        "# NEO4J_USERNAME=neo4j\n",
        "# NEO4J_PASSWORD=3SlhjWFvQum-tM3otagqcdMQ9Au0oENlQUuuSHnRCak\n",
        "# AURA_INSTANCEID=19d8dc72\n",
        "# AURA_INSTANCENAME=ninkspaces"
      ],
      "metadata": {
        "id": "0BXD-xufYSmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"  # Your Neo4j Aura URI\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key (replace with your actual API details)\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'  # Replace with your actual API key\n"
      ],
      "metadata": {
        "id": "GWsjbvgjYKlV"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract(response):\n",
        "    clean_response = re.sub(r'```cypher|```', '', response)\n",
        "    cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "    extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "    if extracted_queries:\n",
        "        # Step 4: Remove any repeated Cypher queries (return only unique ones)\n",
        "        seen_queries = set()\n",
        "        unique_queries = []\n",
        "        for query in extracted_queries:\n",
        "            if query not in seen_queries:\n",
        "                seen_queries.add(query)\n",
        "                unique_queries.append(query)\n",
        "\n",
        "        # Return the first unique query\n",
        "        return unique_queries[0] if unique_queries else None\n",
        "    else:\n",
        "        # Fallback to return the whole response if nothing is split\n",
        "        return clean_response.strip()\n",
        "\n",
        "# Define a function to execute a Cypher query and return the results\n",
        "def run_cypher_query(cypher_query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query)\n",
        "        return result.data()  # Return all records as a list of dictionaries\n"
      ],
      "metadata": {
        "id": "h7zoKd4mbsia"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base prompt as a function to ensure reusability and customization\n",
        "def get_base_prompt():\n",
        "    return '''\n",
        "    You are an expert Cypher query generator for a graph database with the following nodes and relationships:\n",
        "\n",
        "    - Nodes:\n",
        "      - `Seller`: Represents a seller with the following properties: `id`, `name`.\n",
        "      - `Customer`: Represents a customer with the following properties: `id`, `name`.\n",
        "      - `SaleOrder`: Represents a sales order with the following properties: `SONumber`, `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `CostSheet`: Represents a cost sheet with the following properties: `CSNumber`, `Internal`, `External`.\n",
        "      - `Service`: Represents a service with the following properties: `Service`, `Original`.\n",
        "      - `Platform/Technology`: Represents a platform or technology with the following property: `Original`.\n",
        "\n",
        "    - Relationships:\n",
        "      - `HAS_COST_SHEET`: Connects `SaleOrder` to `CostSheet`.\n",
        "      - `PROVIDES_SERVICE`: Connects `SaleOrder` to `Service`.\n",
        "      - `SERVICE_COST`: Connects `CostSheet` to `Service` with properties: `Internal`, `External`.\n",
        "      - `DEPLOYED_ON`: Connects `Service` to `Platform/Technology`.\n",
        "      - `PLACED_ORDER`: Connects `Customer` to `SaleOrder` with properties: `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `HANDLED_ORDER`: Connects `Seller` to `SaleOrder`.\n",
        "\n",
        "    Your task is to generate a **single Cypher query** based on the question.\n",
        "\n",
        "    - Provide only the Cypher query, nothing else.\n",
        "    - Do not provide explanations, markdown syntax, or additional queries.\n",
        "    - Return the Cypher query **once**.\n",
        "    - Do not generate any follow-up questions or instructions.\n",
        "    - Stop after generating the query.\n",
        "    - Ensure that **only a single instance** of the Cypher query code is returned.\n",
        "\n",
        "    ### What to do:\n",
        "    1. **Clean the Response**: The code removes all occurrences of ```cypher and ``` markers.\n",
        "    2. **Split by \"Given the question\"**: It splits the text on \"Given the question:\" so that it ignores anything after it.\n",
        "    3. **Regex for Cypher Query**: It searches for the first occurrence of a Cypher `MATCH` statement and captures everything up to the end of the query (marked by a semicolon `;`).\n",
        "    4. **Returns the First Valid Query**: The function returns the first Cypher query that matches the pattern.\n",
        "\n",
        "    Given the question: \"{question}\"\n",
        "    '''\n",
        "\n",
        "# Define the function that interacts with the LLM and returns a Cypher query\n",
        "def llm_pipeline(question):\n",
        "    baseprompt = get_base_prompt()\n",
        "    formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,  # Adjust based on complexity\n",
        "        'temperature': 0.05,     # Lower temperature for more deterministic output\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0  # Ensures it doesn't repeat or generate unnecessary content\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_cypher_query = extract(response.strip())\n",
        "    return clean_cypher_query\n",
        "\n",
        "start_time = time.time()\n"
      ],
      "metadata": {
        "id": "MNu3fTbL9vCL"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"give me the customer name that have placed the most order and count the total of order\"\n",
        "start_time = time.time()\n",
        "# Generate the Cypher query\n",
        "cypher_query = llm_pipeline(question)\n",
        "print(cypher_query)\n",
        "\n",
        "# Print time taken\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAVr05F9X5lZ",
        "outputId": "8af70af4-d00f-4e7f-e091-c430a3d665fd"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder) RETURN c.name, COUNT(so) as total ORDER BY total DESC LIMIT 1;\n",
            "Time taken: 13.43 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"give me the seller id and name that have handled the most order\"\n",
        "start_time = time.time()\n",
        "# Generate the Cypher query\n",
        "cypher_query = llm_pipeline(question)\n",
        "print(cypher_query)\n",
        "result_record = run_cypher_query(cypher_query)\n",
        "print(result_record)\n",
        "# Print time taken\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "hS5U7O6mI8c-",
        "outputId": "ee9c5871-2274-440a-96d4-161570663ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (s:Seller)-[:HANDLED_ORDER]->(so:SaleOrder) RETURN s.id, s.name, COUNT(so) as order_count ORDER BY order_count DESC LIMIT 1;\n",
            "[{'s.id': '57005', 's.name': 'นางสาวพัชราภรณ์ แนบเนียน', 'order_count': 495}]\n",
            "Time taken: 14.90 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"give me the 5 names of customer that have placed the most order\"\n",
        "start_time = time.time()\n",
        "# Generate the Cypher query\n",
        "cypher_query = llm_pipeline(question)\n",
        "print(cypher_query)\n",
        "result_record = run_cypher_query(cypher_query)\n",
        "print(result_record)\n",
        "# Print time taken\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "tL-m2keVJGW5",
        "outputId": "c0385e79-35fd-4172-94f7-fc9201207fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder) RETURN c.name, COUNT(so) as order_count ORDER BY order_count DESC LIMIT 5;\n",
            "[{'c.name': 'ไอเน็กซ์ บรอดแบนด์', 'order_count': 252}, {'c.name': 'ไทเกอร์ซอฟท์ (1998)', 'order_count': 99}, {'c.name': 'โอเพ่นแลนด์สเคป', 'order_count': 61}, {'c.name': 'อี-บิซิเนส พลัส', 'order_count': 40}, {'c.name': 'ดี.ที.ซี เอ็นเตอร์ไพรส์', 'order_count': 39}]\n",
            "Time taken: 20.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **dynamic llm**"
      ],
      "metadata": {
        "id": "xANs1ekAe5J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_prompt(question, result_record):\n",
        "    return f'''\n",
        "    You are an expert designed to provide clear, concise answers based on query results from a graph database.\n",
        "\n",
        "    ### Instructions: (never copy it)\n",
        "    1. **Understand the question**: \"{question}\"\n",
        "    2. **Review the result data**: {result_record}\n",
        "    3. **Respond with a clear, human-like answer** that directly addresses the question without repetition.\n",
        "\n",
        "    ### Guidelines for the answer: (never copy it)\n",
        "    - Answer **must** be brief, precise, and directly address the question.\n",
        "    - List customers and order counts in a clean, readable format (bullet points are encouraged for clarity).\n",
        "    - **Avoid** any additional commentary, unnecessary text, word repetation, or other unrelated information.\n",
        "    - Does **not** include any additional text, titles, or the word \"assistant.\"\n",
        "    '''\n",
        "def generate_answer_with_llm(question, result_record):\n",
        "    answer_prompt = get_answer_prompt(question, result_record)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 300,  # Adjust based on complexity\n",
        "        'temperature': 0.1,     # Lower temperature for more deterministic output\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0  # Ensures it doesn't repeat or generate unnecessary content\n",
        "    }\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(answer_prompt, **model_params)\n",
        "    final_answer = \"\".join(response)\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "GjmZ3keneLpw"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(question)\n",
        "print(result_record)"
      ],
      "metadata": {
        "id": "V0zQv3S-JyDn",
        "outputId": "d18bbd19-c5f1-49f3-963a-580fd8135166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give me the 5 names of customer that have placed the most order\n",
            "[{'c.name': 'ไอเน็กซ์ บรอดแบนด์', 'order_count': 252}, {'c.name': 'ไทเกอร์ซอฟท์ (1998)', 'order_count': 99}, {'c.name': 'โอเพ่นแลนด์สเคป', 'order_count': 61}, {'c.name': 'อี-บิซิเนส พลัส', 'order_count': 40}, {'c.name': 'ดี.ที.ซี เอ็นเตอร์ไพรส์', 'order_count': 39}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the LLM to generate the final answer\n",
        "final_answer = generate_answer_with_llm(question, result_record)\n",
        "print(final_answer)"
      ],
      "metadata": {
        "id": "coHc_-GkJs0u",
        "outputId": "551635ca-7bd3-4835-a592-f27bfbf84a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "The top 5 customers by order count are:\n",
            "• ไอเน็กซ์ บรอดแบนด์ - 252 orders\n",
            "• ไทเกอร์ซอฟท์ (1998) - 99 orders\n",
            "• โอเพ่นแลนด์สเคป - 61 orders\n",
            "• อี-บิซิเนส พลัส - 40 orders\n",
            "• ดี.ที.ซี เอ็นเตอร์ไพรส์ - 39 orders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **integrated**"
      ],
      "metadata": {
        "id": "PGnAZPV6OOd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "import gradio as gr\n",
        "\n",
        "print(huggingface_hub.__version__)\n",
        "print(gr.__version__)"
      ],
      "metadata": {
        "id": "47r81pvzO0w8",
        "outputId": "3fc46dba-b08a-4e7a-9540-8c99aceca3c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25.1\n",
            "4.44.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from neo4j import GraphDatabase\n",
        "import re\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"  # Your Neo4j Aura URI\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key (replace with your actual API details)\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'  # Replace with your actual API key\n",
        "\n",
        "def extract(response):\n",
        "    clean_response = re.sub(r'```cypher|```', '', response)\n",
        "    clean_response = re.sub(r'^\\*/\\s*', '', clean_response, flags=re.MULTILINE)  # Remove leading \"*/ \"\n",
        "    cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "    extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "    if extracted_queries:\n",
        "        # Remove any repeated Cypher queries (return only unique ones)\n",
        "        seen_queries = set()\n",
        "        unique_queries = []\n",
        "        for query in extracted_queries:\n",
        "            if query not in seen_queries:\n",
        "                seen_queries.add(query)\n",
        "                unique_queries.append(query)\n",
        "\n",
        "        # Return the first unique query\n",
        "        return unique_queries[0] if unique_queries else None\n",
        "    else:\n",
        "        # Fallback to return the whole response if nothing is split\n",
        "        return clean_response.strip()\n",
        "\n",
        "# Define a function to execute a Cypher query and return the results\n",
        "def run_cypher_query(cypher_query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query)\n",
        "        return result.data()  # Return all records as a list of dictionaries\n",
        "\n",
        "def get_base_prompt():\n",
        "    return '''\n",
        "    You are an expert Cypher query generator for a graph database with the following nodes and relationships:\n",
        "\n",
        "    - Nodes:\n",
        "      - `Seller`: Represents a seller with the following properties: `id`, `name`.\n",
        "      - `Customer`: Represents a customer with the following properties: `id`, `name`.\n",
        "      - `SaleOrder`: Represents a sales order with the following properties: `SONumber`, `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `CostSheet`: Represents a cost sheet with the following properties: `CSNumber`, `Internal`, `External`.\n",
        "      - `Service`: Represents a service with the following properties: `Service`, `Original`.\n",
        "      - `Platform/Technology`: Represents a platform or technology with the following property: `Original`.\n",
        "\n",
        "    - Relationships:\n",
        "      - `HAS_COST_SHEET`: Connects `SaleOrder` to `CostSheet`.\n",
        "      - `PROVIDES_SERVICE`: Connects `SaleOrder` to `Service`.\n",
        "      - `SERVICE_COST`: Connects `CostSheet` to `Service` with properties: `Internal`, `External`.\n",
        "      - `DEPLOYED_ON`: Connects `Service` to `Platform/Technology`.\n",
        "      - `PLACED_ORDER`: Connects `Customer` to `SaleOrder` with properties: `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `HANDLED_ORDER`: Connects `Seller` to `SaleOrder`.\n",
        "\n",
        "    Your task is to generate a **single Cypher query** based on the question.\n",
        "\n",
        "    - Provide only the Cypher query, nothing else.\n",
        "    - Do not provide explanations, markdown syntax, or additional queries.\n",
        "    - Return the Cypher query **once**.\n",
        "    - Do not generate any follow-up questions or instructions.\n",
        "    - Stop after generating the query.\n",
        "    - Ensure that **only a single instance** of the Cypher query code is returned.\n",
        "\n",
        "    Given the question: \"{question}\"\n",
        "    '''\n",
        "\n",
        "def llm_pipeline(question):\n",
        "    baseprompt = get_base_prompt()\n",
        "    formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,  # Adjust based on complexity\n",
        "        'temperature': 0.05,     # Lower temperature for more deterministic output\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0  # Ensures it doesn't repeat or generate unnecessary content\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_cypher_query = extract(response.strip())\n",
        "    return clean_cypher_query\n",
        "\n",
        "def get_answer_prompt(question, result_record):\n",
        "    return f'''\n",
        "    You are an expert designed to provide clear and concise answers based on query results from a graph database.\n",
        "\n",
        "    ### Instructions:\n",
        "    1. Understand the question: \"{question}\"\n",
        "    2. Review the result data: {result_record}\n",
        "    3. Respond with a clear, direct answer without any extra commentary, unnecessary text, or formatting symbols.\n",
        "    4. The reponse must **complete**.\n",
        "\n",
        "    ### Guidelines for the answer:\n",
        "    - Provide the answer in a straightforward manner.\n",
        "    - Use bullet points for lists where appropriate.\n",
        "    - Do not include any additional commentary, explanations, or the word \"assistant.\"\n",
        "    - Do not provide any symbols like \"/\" and any internal processing details or chain of thought in the output.\n",
        "    '''\n",
        "\n",
        "def generate_answer_with_llm(question, result_record):\n",
        "    answer_prompt = get_answer_prompt(question, result_record)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,\n",
        "        'temperature': 0.1,\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(answer_prompt, **model_params)\n",
        "    clean_response = response.strip().replace('*/', '').replace('```', '').replace('```cypher', '')\n",
        "    return clean_response.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "kglO-_MnOTNw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Function to run a Cypher query\n",
        "def run_cypher_query(cypher_query):\n",
        "    print(f\"Executing Cypher Query: {cypher_query}\")  # Print query for debugging\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query)\n",
        "        return result.data()  # Return all records as a list of dictionaries\n",
        "\n",
        "# Function to handle chatbot response\n",
        "def chatbot_response(message, chat_history):\n",
        "    cypher_query = llm_pipeline(message)\n",
        "    if cypher_query:\n",
        "        result_record = run_cypher_query(cypher_query)\n",
        "        if result_record:\n",
        "            answer = generate_answer_with_llm(message, result_record)\n",
        "            chat_history.append((message, answer))\n",
        "            print(f\"Generated Cypher Query (Debug): {cypher_query}\")\n",
        "        else:\n",
        "            chat_history.append((message, \"No relevant data found in the database.\"))\n",
        "    else:\n",
        "        chat_history.append((message, \"Failed to generate a valid Cypher query.\"))\n",
        "\n",
        "    return \"\", chat_history\n",
        "\n",
        "# Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot_ui = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Ask your question about sales...\")\n",
        "    clear = gr.ClearButton([msg, chatbot_ui])\n",
        "\n",
        "    # Submit message and get response\n",
        "    msg.submit(chatbot_response, [msg, chatbot_ui], [msg, chatbot_ui])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "TCRLLIBvOeEn",
        "outputId": "cef6b1dc-545f-4666-a4c7-66cdee96a76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://581c8ca02ee677aa44.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://581c8ca02ee677aa44.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in '__init__': pass token='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' as keyword args. From version 0.26 passing these as positional arguments will result in an error,\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Cypher Query: MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder)\n",
            "RETURN c.name, COUNT(so) as order_count\n",
            "ORDER BY order_count DESC\n",
            "LIMIT 1;\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://581c8ca02ee677aa44.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#give me the names of customer that have placed the most order"
      ],
      "metadata": {
        "id": "x-PpyaVmP0nc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}