{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/neo4j/graph_database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install huggingface_hub transformers langchain langchain-community neo4j requests gradio torch"
      ],
      "metadata": {
        "id": "uMLiyiHgX0aq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefb5983-cbd5-439b-f0b1-b62899848ddd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 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\n",
        "# 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\n",
        "# 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\n",
        "# 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\n",
        "\n",
        "# NEO4J_URI=neo4j+s://19d8dc72.databases.neo4j.io\n",
        "# NEO4J_USERNAME=neo4j\n",
        "# NEO4J_PASSWORD=3SlhjWFvQum-tM3otagqcdMQ9Au0oENlQUuuSHnRCak\n",
        "# AURA_INSTANCEID=19d8dc72\n",
        "# AURA_INSTANCENAME=ninkspaces"
      ],
      "metadata": {
        "id": "0BXD-xufYSmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from neo4j import GraphDatabase\n",
        "import time\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"  # Your Neo4j Aura URI\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key (replace with your actual API details)\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'  # Replace with your actual API key\n"
      ],
      "metadata": {
        "id": "GWsjbvgjYKlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract(response):\n",
        "    clean_response = re.sub(r'```cypher|```', '', response)\n",
        "    cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "    extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "    if extracted_queries:\n",
        "        # Step 4: Remove any repeated Cypher queries (return only unique ones)\n",
        "        seen_queries = set()\n",
        "        unique_queries = []\n",
        "        for query in extracted_queries:\n",
        "            if query not in seen_queries:\n",
        "                seen_queries.add(query)\n",
        "                unique_queries.append(query)\n",
        "\n",
        "        # Return the first unique query\n",
        "        return unique_queries[0] if unique_queries else None\n",
        "    else:\n",
        "        # Fallback to return the whole response if nothing is split\n",
        "        return clean_response.strip()\n",
        "\n",
        "# Define a function to execute a Cypher query and return the results\n",
        "def run_cypher_query(cypher_query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query)\n",
        "        return result.data()  # Return all records as a list of dictionaries\n"
      ],
      "metadata": {
        "id": "h7zoKd4mbsia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base prompt as a function to ensure reusability and customization\n",
        "def get_base_prompt():\n",
        "    return '''\n",
        "    You are an expert Cypher query generator for a graph database with the following nodes and relationships:\n",
        "\n",
        "    - Nodes:\n",
        "      - `Seller`: Represents a seller with the following properties: `id`, `name`.\n",
        "      - `Customer`: Represents a customer with the following properties: `id`, `name`.\n",
        "      - `SaleOrder`: Represents a sales order with the following properties: `SONumber`, `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `CostSheet`: Represents a cost sheet with the following properties: `CSNumber`, `Internal`, `External`.\n",
        "      - `Service`: Represents a service with the following properties: `Service`, `Original`.\n",
        "      - `Platform`: Represents a platform or technology with the following property: `Original`.\n",
        "\n",
        "    - Relationships:\n",
        "      - `HAS_COST_SHEET`: Connects `SaleOrder` to `CostSheet`.\n",
        "      - `PROVIDES_SERVICE`: Connects `SaleOrder` to `Service`.\n",
        "      - `SERVICE_COST`: Connects `CostSheet` to `Service` with properties: `Internal`, `External`.\n",
        "      - `DEPLOYED_ON`: Connects `Service` to `Platform`.\n",
        "      - `PLACED_ORDER`: Connects `Customer` to `SaleOrder` with properties: `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `HANDLED_ORDER`: Connects `Seller` to `SaleOrder`.\n",
        "\n",
        "    Your task is to generate a **single Cypher query** based on the question.\n",
        "\n",
        "    - Provide only the Cypher query, nothing else.\n",
        "    - Do not provide explanations, markdown syntax, or additional queries.\n",
        "    - Return the Cypher query **once**.\n",
        "    - Do not generate any follow-up questions or instructions.\n",
        "    - Stop after generating the query.\n",
        "    - Ensure that **only a single instance** of the Cypher query code is returned.\n",
        "\n",
        "    ### What to do:\n",
        "    1. **Clean the Response**: The code removes all occurrences of ```cypher and ``` markers.\n",
        "    2. **Split by \"Given the question\"**: It splits the text on \"Given the question:\" so that it ignores anything after it.\n",
        "    3. **Regex for Cypher Query**: It searches for the first occurrence of a Cypher `MATCH` statement and captures everything up to the end of the query (marked by a semicolon `;`).\n",
        "    4. **Returns the First Valid Query**: The function returns the first Cypher query that matches the pattern.\n",
        "\n",
        "    Given the question: \"{question}\"\n",
        "    '''\n",
        "\n",
        "# Define the function that interacts with the LLM and returns a Cypher query\n",
        "def llm_pipeline(question):\n",
        "    baseprompt = get_base_prompt()\n",
        "    formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,  # Adjust based on complexity\n",
        "        'temperature': 0.05,     # Lower temperature for more deterministic output\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0  # Ensures it doesn't repeat or generate unnecessary content\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_cypher_query = extract(response.strip())\n",
        "    return clean_cypher_query\n"
      ],
      "metadata": {
        "id": "MNu3fTbL9vCL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"give me the customer name that have placed the most order and count the total of order\"\n",
        "start_time = time.time()\n",
        "# Generate the Cypher query\n",
        "cypher_query = llm_pipeline(question)\n",
        "print(cypher_query)\n",
        "\n",
        "# Print time taken\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAVr05F9X5lZ",
        "outputId": "8af70af4-d00f-4e7f-e091-c430a3d665fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder) RETURN c.name, COUNT(so) as total ORDER BY total DESC LIMIT 1;\n",
            "Time taken: 13.43 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"give me the seller id and name that have handled the most order\"\n",
        "start_time = time.time()\n",
        "# Generate the Cypher query\n",
        "cypher_query = llm_pipeline(question)\n",
        "print(cypher_query)\n",
        "result_record = run_cypher_query(cypher_query)\n",
        "print(result_record)\n",
        "# Print time taken\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "hS5U7O6mI8c-",
        "outputId": "ee9c5871-2274-440a-96d4-161570663ba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (s:Seller)-[:HANDLED_ORDER]->(so:SaleOrder) RETURN s.id, s.name, COUNT(so) as order_count ORDER BY order_count DESC LIMIT 1;\n",
            "[{'s.id': '57005', 's.name': 'นางสาวพัชราภรณ์ แนบเนียน', 'order_count': 495}]\n",
            "Time taken: 14.90 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"give me the 5 names of customer that have placed the most order\"\n",
        "start_time = time.time()\n",
        "# Generate the Cypher query\n",
        "cypher_query = llm_pipeline(question)\n",
        "print(cypher_query)\n",
        "result_record = run_cypher_query(cypher_query)\n",
        "print(result_record)\n",
        "# Print time taken\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "tL-m2keVJGW5",
        "outputId": "c0385e79-35fd-4172-94f7-fc9201207fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MATCH (c:Customer)-[:PLACED_ORDER]->(so:SaleOrder) RETURN c.name, COUNT(so) as order_count ORDER BY order_count DESC LIMIT 5;\n",
            "[{'c.name': 'ไอเน็กซ์ บรอดแบนด์', 'order_count': 252}, {'c.name': 'ไทเกอร์ซอฟท์ (1998)', 'order_count': 99}, {'c.name': 'โอเพ่นแลนด์สเคป', 'order_count': 61}, {'c.name': 'อี-บิซิเนส พลัส', 'order_count': 40}, {'c.name': 'ดี.ที.ซี เอ็นเตอร์ไพรส์', 'order_count': 39}]\n",
            "Time taken: 20.04 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **dynamic llm**"
      ],
      "metadata": {
        "id": "xANs1ekAe5J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_prompt(question, result_record):\n",
        "    return f'''\n",
        "    You are an expert designed to provide clear, concise answers based on query results from a graph database.\n",
        "\n",
        "    ### Instructions: (never copy it)\n",
        "    1. **Understand the question**: \"{question}\"\n",
        "    2. **Review the result data**: {result_record}\n",
        "    3. **Respond with a clear, human-like answer** that directly addresses the question without repetition.\n",
        "\n",
        "    ### Guidelines for the answer: (never copy it)\n",
        "    - Answer **must** be brief, precise, and directly address the question.\n",
        "    - List customers and order counts in a clean, readable format (bullet points are encouraged for clarity).\n",
        "    - **Avoid** any additional commentary, unnecessary text, word repetation, or other unrelated information.\n",
        "    - Does **not** include any additional text, titles, or the word \"assistant.\"\n",
        "    '''\n",
        "def generate_answer_with_llm(question, result_record):\n",
        "    answer_prompt = get_answer_prompt(question, result_record)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 300,  # Adjust based on complexity\n",
        "        'temperature': 0.1,     # Lower temperature for more deterministic output\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0  # Ensures it doesn't repeat or generate unnecessary content\n",
        "    }\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(answer_prompt, **model_params)\n",
        "    final_answer = \"\".join(response)\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "GjmZ3keneLpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(question)\n",
        "print(result_record)"
      ],
      "metadata": {
        "id": "V0zQv3S-JyDn",
        "outputId": "d18bbd19-c5f1-49f3-963a-580fd8135166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give me the 5 names of customer that have placed the most order\n",
            "[{'c.name': 'ไอเน็กซ์ บรอดแบนด์', 'order_count': 252}, {'c.name': 'ไทเกอร์ซอฟท์ (1998)', 'order_count': 99}, {'c.name': 'โอเพ่นแลนด์สเคป', 'order_count': 61}, {'c.name': 'อี-บิซิเนส พลัส', 'order_count': 40}, {'c.name': 'ดี.ที.ซี เอ็นเตอร์ไพรส์', 'order_count': 39}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the LLM to generate the final answer\n",
        "final_answer = generate_answer_with_llm(question, result_record)\n",
        "print(final_answer)"
      ],
      "metadata": {
        "id": "coHc_-GkJs0u",
        "outputId": "551635ca-7bd3-4835-a592-f27bfbf84a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "The top 5 customers by order count are:\n",
            "• ไอเน็กซ์ บรอดแบนด์ - 252 orders\n",
            "• ไทเกอร์ซอฟท์ (1998) - 99 orders\n",
            "• โอเพ่นแลนด์สเคป - 61 orders\n",
            "• อี-บิซิเนส พลัส - 40 orders\n",
            "• ดี.ที.ซี เอ็นเตอร์ไพรส์ - 39 orders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **integrated**"
      ],
      "metadata": {
        "id": "PGnAZPV6OOd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "import gradio as gr\n",
        "\n",
        "print(huggingface_hub.__version__)\n",
        "print(gr.__version__)"
      ],
      "metadata": {
        "id": "47r81pvzO0w8",
        "outputId": "aba07c6c-4330-4903-bf0a-269d73dfd3fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.24.7\n",
            "4.44.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_prompt():\n",
        "    return '''\n",
        "    You are an expert Cypher query generator for a graph database with the following nodes and relationships:\n",
        "\n",
        "    - Nodes:\n",
        "      - `Seller`: Represents a seller with the following properties: `id`, `name`.\n",
        "      - `Customer`: Represents a customer with the following properties: `id`, `name`.\n",
        "      - `SaleOrder`: Represents a sales order with the following properties: `SONumber`, `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `CostSheet`: Represents a cost sheet with the following properties: `CSNumber`, `Internal`, `External`.\n",
        "      - `Service`: Represents a service with the following properties: `Service`, `Original`.\n",
        "      - `Platform`: Represents a platform or technology with the following property: `Original`.\n",
        "\n",
        "    - Relationships:\n",
        "      - `HAS_COST_SHEET`: Connects `SaleOrder` to `CostSheet`.\n",
        "      - `PROVIDES_SERVICE`: Connects `SaleOrder` to `Service`.\n",
        "      - `SERVICE_COST`: Connects `CostSheet` to `Service` with properties: `Internal`, `External`.\n",
        "      - `DEPLOYED_ON`: Connects `Service` to `Platform`.\n",
        "      - `PLACED_ORDER`: Connects `Customer` to `SaleOrder` with properties: `ContractStartDate`, `ContractEndDate`, `Total`.\n",
        "      - `HANDLED_ORDER`: Connects `Seller` to `SaleOrder`.\n",
        "\n",
        "    Your task is to generate a **single Cypher query** based on the question.\n",
        "\n",
        "    - Provide only the Cypher query, nothing else.\n",
        "    - Do not provide explanations, markdown syntax, or additional queries.\n",
        "    - Return the Cypher query **once**.\n",
        "    - Do not generate any follow-up questions or instructions.\n",
        "    - Stop after generating the query.\n",
        "    - Do not include code block, any additional text symbol, titles, or the word \"assistant\" and \"/\".\n",
        "    - Ensure that **only a single instance** of the Cypher query code is returned.\n",
        "\n",
        "    Given the question: {question}\n",
        "    '''\n",
        "\n",
        "def get_answer_prompt():\n",
        "    return '''\n",
        "    You are a data extraction assistant specialized in providing direct answers based solely on results from a graph database.\n",
        "\n",
        "    **Task:** Provide the answer below using only the information from the result data provided.\n",
        "\n",
        "    **Question:** {question}\n",
        "\n",
        "    **Result Data:** {result_record}\n",
        "\n",
        "    - Respond with only the answer.\n",
        "    - Do not inlude step of thinking process.\n",
        "    - Do not include any explanations, reasoning, or additional commentary.\n",
        "    - If the answer is a list, present all items in the list.\n",
        "    - If there is no relevant data, respond with \"No data found.\"\n",
        "    - Do not include code block, any additional text symbol, titles, or the word \"assistant\" and \"/\".\n",
        "    '''"
      ],
      "metadata": {
        "id": "kglO-_MnOTNw"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import asyncio\n",
        "import re\n",
        "from huggingface_hub import InferenceClient\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key (replace with your actual API details)\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'\n",
        "\n",
        "def extract(response):\n",
        "    clean_response = re.sub(r'```cypher|```', '', response)\n",
        "    clean_response = re.sub(r'^\\*/\\s*', '', clean_response, flags=re.MULTILINE)\n",
        "    cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "    extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "    if extracted_queries:\n",
        "        seen_queries = set()\n",
        "        unique_queries = []\n",
        "        for query in extracted_queries:\n",
        "            if query not in seen_queries:\n",
        "                seen_queries.add(query)\n",
        "                unique_queries.append(query)\n",
        "        return unique_queries[0] if unique_queries else None\n",
        "    else:\n",
        "        return clean_response.strip()\n",
        "\n",
        "def run_cypher_query(cypher_query, limit=10):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(f\"{cypher_query} LIMIT {limit}\")\n",
        "        return result.data()  # Return paginated records as a list of dictionaries\n",
        "\n",
        "# Function to format result records into a string\n",
        "def format_result_record(result_record):\n",
        "    if isinstance(result_record, list):\n",
        "        return \"\\n\".join([str(record) for record in result_record])\n",
        "    return str(result_record)\n",
        "\n",
        "async def generate_answer_with_llm(question, result_record):\n",
        "    formatted_result = format_result_record(result_record)  # Format result data appropriately\n",
        "    answer_prompt = get_answer_prompt()\n",
        "    formatted_prompt = answer_prompt.replace(\"{question}\", question).replace(\"{result_record}\", formatted_result)\n",
        "\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,\n",
        "        'temperature': 0.01,\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_response = extract(response.strip())\n",
        "    clean_response = clean_response.strip()\n",
        "\n",
        "    # Final check for empty responses\n",
        "    if clean_response == \"\":\n",
        "        return \"No data found.\"\n",
        "\n",
        "    return clean_response\n",
        "\n",
        "# Synchronous call to LLM inference\n",
        "def llm_pipeline(question):\n",
        "    baseprompt = get_base_prompt()\n",
        "    formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,\n",
        "        'temperature': 0.01,\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_cypher_query = extract(response.strip())\n",
        "    return clean_cypher_query\n"
      ],
      "metadata": {
        "id": "TCRLLIBvOeEn"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle chatbot response\n",
        "async def chatbot_response(message, chat_history):\n",
        "    cypher_query = llm_pipeline(message)\n",
        "    print(cypher_query)  # Debugging print\n",
        "    if cypher_query:\n",
        "        result_record = run_cypher_query(cypher_query)\n",
        "        if result_record:\n",
        "            answer = await generate_answer_with_llm(message, result_record)  # Await the async function\n",
        "            chat_history.append((message, answer))\n",
        "        else:\n",
        "            chat_history.append((message, \"No relevant data found in the database.\"))\n",
        "    else:\n",
        "        chat_history.append((message, \"Failed to generate a valid Cypher query.\"))\n",
        "\n",
        "    return \"\", chat_history\n",
        "\n",
        "# Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    # Set a narrower width for the components\n",
        "    chatbot_ui = gr.Chatbot(label=\"Chatbot\")\n",
        "    msg = gr.Textbox(placeholder=\"Ask a question about the cost sheet...\")\n",
        "    clear = gr.ClearButton([msg, chatbot_ui])\n",
        "\n",
        "    # Submit message and get response\n",
        "    msg.submit(chatbot_response, [msg, chatbot_ui], [msg, chatbot_ui])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "Xn3RrzqMsUHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **visualize graph**"
      ],
      "metadata": {
        "id": "eXECi8Sajg8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pyvis networkx"
      ],
      "metadata": {
        "id": "Yq0C0NNrjt-S"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import asyncio\n",
        "import re\n",
        "from huggingface_hub import InferenceClient\n",
        "from neo4j import GraphDatabase\n",
        "from pyvis.network import Network\n",
        "import pandas as pd\n",
        "\n",
        "# Neo4j database connection credentials\n",
        "NEO4J_URI = \"neo4j+s://ba8feaac.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"P5vvwJNewVk42Ey31ynvL9vrRRx98vlmv_5NnmVtshw\"\n",
        "\n",
        "# Define the Neo4j driver connection\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
        "\n",
        "# Define your LLM API endpoint and key (replace with your actual API details)\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-03/'\n",
        "api_key = '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'\n",
        "\n",
        "def extract(response):\n",
        "    clean_response = re.sub(r'```cypher|```', '', response)\n",
        "    clean_response = re.sub(r'^\\*/\\s*', '', clean_response, flags=re.MULTILINE)\n",
        "    cypher_queries = re.split(r'Given the question:', clean_response)\n",
        "    extracted_queries = [query.strip() for query in cypher_queries if query.strip()]\n",
        "\n",
        "    if extracted_queries:\n",
        "        seen_queries = set()\n",
        "        unique_queries = []\n",
        "        for query in extracted_queries:\n",
        "            if query not in seen_queries:\n",
        "                seen_queries.add(query)\n",
        "                unique_queries.append(query)\n",
        "        return unique_queries[0] if unique_queries else None\n",
        "    else:\n",
        "        return clean_response.strip()\n",
        "\n",
        "# Function to run a Cypher query\n",
        "def run_cypher_query(cypher_query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(cypher_query)\n",
        "        return result.data()  # Return all records as a list of dictionaries\n",
        "\n",
        "# Function to format result records into a string\n",
        "def format_result_record(result_record):\n",
        "    if isinstance(result_record, list):\n",
        "        return \"\\n\".join([str(record) for record in result_record])\n",
        "    return str(result_record)\n",
        "\n",
        "async def generate_answer_with_llm(question, result_record):\n",
        "    formatted_result = format_result_record(result_record)  # Format result data appropriately\n",
        "    answer_prompt = get_answer_prompt()\n",
        "    formatted_prompt = answer_prompt.replace(\"{question}\", question).replace(\"{result_record}\", formatted_result)\n",
        "\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,\n",
        "        'temperature': 0.01,\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_response = extract(response.strip())\n",
        "    clean_response = clean_response.strip()\n",
        "\n",
        "    # Final check for empty responses\n",
        "    if clean_response == \"\":\n",
        "        return \"No data found.\"\n",
        "\n",
        "    return clean_response\n",
        "\n",
        "# Synchronous call to LLM inference\n",
        "def llm_pipeline(question):\n",
        "    baseprompt = get_base_prompt()\n",
        "    formatted_prompt = baseprompt.replace(\"{question}\", question)\n",
        "    model_params = {\n",
        "        'max_new_tokens': 512,\n",
        "        'temperature': 0.01,\n",
        "        'top_p': 0.95,\n",
        "        'repetition_penalty': 1.0\n",
        "    }\n",
        "\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    clean_cypher_query = extract(response.strip())\n",
        "    return clean_cypher_query\n",
        "\n",
        "from pyvis.network import Network\n",
        "\n",
        "def visualize_graph(data):\n",
        "    net = Network(height='600px', width='100%', notebook=True)\n",
        "\n",
        "    # Add nodes and edges to the network\n",
        "    for record in data:\n",
        "        # Extract nodes\n",
        "        customer = record.get('c')\n",
        "        sale_order = record.get('so')\n",
        "        cost_sheet = record.get('cs')\n",
        "        service = record.get('s')\n",
        "        platform = record.get('p')\n",
        "\n",
        "        # Create nodes\n",
        "        if customer:\n",
        "            net.add_node(customer['id'], label=customer['name'], color='blue')\n",
        "        if sale_order:\n",
        "            net.add_node(sale_order['SONumber'], label=sale_order['SONumber'], color='orange')\n",
        "        if cost_sheet:\n",
        "            net.add_node(cost_sheet['CSNumber'], label=cost_sheet['CSNumber'], color='green')\n",
        "        if service:\n",
        "            net.add_node(service['Original'], label=service['Original'], color='red')\n",
        "        if platform:\n",
        "            net.add_node(platform['Original'], label=platform['Original'], color='purple')\n",
        "\n",
        "        # Create edges\n",
        "        if customer and sale_order:\n",
        "            net.add_edge(customer['id'], sale_order['SONumber'], title='PLACED_ORDER')\n",
        "        if sale_order and cost_sheet:\n",
        "            net.add_edge(sale_order['SONumber'], cost_sheet['CSNumber'], title='HAS_COST_SHEET')\n",
        "        if cost_sheet and service:\n",
        "            net.add_edge(cost_sheet['CSNumber'], service['Original'], title='SERVICE_COST')\n",
        "        if service and platform:\n",
        "            net.add_edge(service['Original'], platform['Original'], title='DEPLOYED_ON')\n",
        "\n",
        "    # Save to HTML file\n",
        "    net.show('graph.html')\n",
        "    return 'graph.html'\n",
        "\n",
        "\n",
        "async def chatbot_response(message, chat_history):\n",
        "    cypher_query = llm_pipeline(message)\n",
        "    print(cypher_query)  # Debugging print\n",
        "    if cypher_query:\n",
        "        result_record = run_cypher_query(cypher_query)\n",
        "        if result_record:\n",
        "            answer = await generate_answer_with_llm(message, result_record)  # Await the async function\n",
        "\n",
        "            # Visualize the graph and get the HTML file\n",
        "            graph_file = visualize_graph(result_record)\n",
        "            with open(graph_file, 'r') as f:\n",
        "                graph_html = f.read()\n",
        "\n",
        "            chat_history.append((message, answer))\n",
        "            chat_history.append((graph_html, \"Here is the graph visualization.\"))\n",
        "        else:\n",
        "            chat_history.append((message, \"No relevant data found in the database.\"))\n",
        "    else:\n",
        "        chat_history.append((message, \"Failed to generate a valid Cypher query.\"))\n",
        "\n",
        "    return \"\", chat_history\n",
        "\n",
        "\n",
        "# Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    # Set a narrower width for the components\n",
        "    chatbot_ui = gr.Chatbot(label=\"Chatbot\")\n",
        "    msg = gr.Textbox(placeholder=\"Ask a question about the cost sheet...\")\n",
        "    clear = gr.ClearButton([msg, chatbot_ui])\n",
        "\n",
        "    # Submit message and get response\n",
        "    msg.submit(chatbot_response, [msg, chatbot_ui], [msg, chatbot_ui])\n",
        "\n",
        "    gr.HTML(value=\"\")  # Placeholder for displaying the graph visualization\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "XrepirksmXYX",
        "outputId": "9eee132f-f2a7-4348-c6f0-cc9e7659ba79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f005215aae163b86be.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f005215aae163b86be.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in '__init__': pass token='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' as keyword args. From version 0.26 passing these as positional arguments will result in an error,\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "give me the information about the customer name \"ไอเน็กซ์ บรอดแบนด์\""
      ],
      "metadata": {
        "id": "x-PpyaVmP0nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **chat history**\n",
        "To enhance the functionality of your chatbot so it **can respond to questions about the chat history**, such as retrieving specific customer information\n",
        "\n",
        "To create a more dynamic chatbot that can intelligently **decide when to query the database and when to simply respond to a conversational follow-up**, we need a context-aware control flow"
      ],
      "metadata": {
        "id": "htXYLFgY06jI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zecv4-iXXuWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}