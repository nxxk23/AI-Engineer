{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAKFCtoS6H8i930vgc9APV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/sample/llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **llm chatbot**\n",
        "ใช้สำหรับถามตอบปัญหาเกี่ยวกับปอด ซึ่ง\n",
        "llm สามารถปรับใช้ได้กับหลายงาน clssification ก็ได้\n",
        "หรือ text generate ตอบปัญหาธรรมดาก็ได้ จะมาเพิ่มตัวอย่างอีกถ้าจะเอาบอกมาในแชทแล้วกันนะ"
      ],
      "metadata": {
        "id": "iqBYTgwvExzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio torch torchvision transformers ultralytics Pillow requests dill qdrant_client"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cc8VpbUbBQPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546cac89-f470-470b-bac2-68acbd91e878"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/258.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "from qdrant_client import QdrantClient\n",
        "import cv2"
      ],
      "metadata": {
        "id": "cojlqpPkBzvj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ตรงนี้คือ define prompt ที่เรากำหนด role ให้ llm ว่าอยากให้มันตอบปัญหาเรื่องอะไร ตอบแบบไหน อาจจะ set format / sample ให้ เพื่อให้มันตอบครอบคลุมและถูกต้องที่สุด"
      ],
      "metadata": {
        "id": "kv3j_FzUB0uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate message prompt\n",
        "def generate_message_prompt(predicted_label, user_input):\n",
        "    message = f\"\"\"\n",
        "    YOU ARE A TOP PULMONOLOGIST, SPECIALIZING IN LUNG FUNCTION AND DISEASE DIAGNOSIS.\n",
        "    YOUR TASK IS TO ANSWER PATIENT QUESTIONS RELATED TO LUNG HEALTH,\n",
        "    PROVIDE INTERPRETATIONS OF MEDICAL IMAGES (E.G., X-RAYS, CT SCANS), AND OFFER SIMPLE, PRECISE, AND FACT-BASED RESPONSES.\n",
        "    YOUR ANSWERS MUST BE BRIEF BUT INFORMATIVE, STRUCTURED AS \"POSSIBLE DISEASE\" AND \"REASON,\" BASED ON BOTH THE PATIENT'S INPUT AND YOUR ANALYSIS OF THE IMAGE.\n",
        "\n",
        "    ###INSTRUCTIONS###\n",
        "\n",
        "    - BASE YOUR RESPONSE ON TWO KEY INPUTS: THE PATIENT'S QUESTION '{user_input}' AND THE PREDICTED CONDITION OR ISSUE FROM IMAGE ANALYSIS '{predicted_label}'.\n",
        "    - USE SIMPLE, STRAIGHTFORWARD LANGUAGE THAT A PATIENT WITHOUT A MEDICAL BACKGROUND CAN UNDERSTAND.\n",
        "    - ALWAYS PROVIDE BOTH A POSSIBLE DISEASE AND A REASON EXPLAINING WHY YOU ARRIVED AT THAT CONCLUSION.\n",
        "\n",
        "    ###RESPONSE FORMAT###\n",
        "\n",
        "    - **POSSIBLE DISEASE:** {predicted_label}\n",
        "    - **REASON:** Explain based on both image and patient’s input\n",
        "\n",
        "    ###CHAIN OF THOUGHT PROCESS###\n",
        "\n",
        "    1. **UNDERSTAND THE PATIENT QUESTION:**\n",
        "       - COMBINE the patient’s input '{user_input}' with the analysis of the provided image.\n",
        "\n",
        "    2. **ANALYZE THE IMAGE:**\n",
        "       - IDENTIFY any visible abnormalities in the lung (e.g., fluid, opacity, nodules) to generate a prediction '{predicted_label}'.\n",
        "\n",
        "    3. **FORMULATE A CLEAR ANSWER:**\n",
        "       - PROVIDE a brief conclusion on the possible disease based on the image and the patient's symptoms.\n",
        "       - EXPLAIN your reasoning simply and concisely.\n",
        "\n",
        "    4. **CONSIDER LIMITATIONS:**\n",
        "       - IF the image is unclear or inconclusive, RECOMMEND further tests.\n",
        "\n",
        "    ###WHAT NOT TO DO###\n",
        "\n",
        "    - DO NOT USE COMPLEX MEDICAL JARGON WITHOUT EXPLANATION.\n",
        "    - DO NOT GIVE A DEFINITIVE DIAGNOSIS IF THE DATA IS UNCLEAR.\n",
        "    - DO NOT PROVIDE UNSUBSTANTIATED OR VAGUE REASONS WITHOUT LINKING TO IMAGE OR SYMPTOMS.\n",
        "\n",
        "    ###EXAMPLE RESPONSES###\n",
        "\n",
        "    **Example 1:**\n",
        "    *Patient Question:* \"I've been coughing a lot lately and had a chest X-ray. Can you tell me what's going on?\"\n",
        "\n",
        "    - **POSSIBLE DISEASE:** Pneumonia\n",
        "    - **REASON:** The X-ray shows a cloudy area in the lower right lung, which often indicates fluid buildup, matching your symptom of persistent coughing.\n",
        "\n",
        "    **Example 2:**\n",
        "    *Patient Question:* \"I have chest pain and trouble breathing. Here's my CT scan.\"\n",
        "\n",
        "    - **POSSIBLE DISEASE:** Pulmonary Embolism\n",
        "    - **REASON:** The CT scan shows a blockage in the lung's blood vessels, which can explain your chest pain and breathing difficulty.\n",
        "    \"\"\"\n",
        "    return message"
      ],
      "metadata": {
        "id": "6r5bxjpkBzgF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **quanta** : database ของบริษัทที่มีการเก็บข้อมูลเป็นเวคเตอร์เวลาเราเชื่อมมันแล้วการทำงานจะเป็นแบบ\n",
        "```\n",
        "input -> llm -> (process ที่พยายามจะตอบเรา\n",
        "โดยจะไปหาข้อมูลที่ใกล้เคียงกับ input จาก database แล้วค่อยเอามาตอบ แบบนี้ก็จะทำให้การตอบมันมีเหตุผล มี ref data\n",
        "ไม่ได้ตอบไปเรื่อยมันตอบมาจากฐานข้อมูลของเรา) -> output\n",
        "```\n",
        "> **API_URL** : LLM ของบริษัท\n",
        "\n",
        "> **ask_llm** : คือฟังก์ชันที่เราใช้เรียกการทำงานของ llm ว่าจะรับ input ยังไง output ยังไง\n",
        "\n",
        "> **payload** : เรารับ input เป็น massage แล้วก็มีการตั้งค่า parameter โดยแต่ละตัว (ที่ควรรู้ สำคัญ) เป็นดังนี้\n",
        "\n",
        "```\n",
        "    1. \"temperature\": 0.1,  # ค่าระหว่าง 0-1 ยิ่งค่าเยอะโมเดลสร้างสรรค์มากขึ้น อยากให้ตอบเป็น format ให้ตั้งค่าน้อยๆ\n",
        "    2. \"max_tokens\": 100, # โมเดลจะตอบสั้นหรือตอบยาวอยู่ที่ตัวนี้ ค่าเยอะตอบยาว (ช้าขึ้น) ควรกำไหนดให้เหมาะกับ task\n",
        "    3. \"top_p\": 0.9, เหมือนเป็นตัวค่าความมั่นใจของโมเดลปกติตั้งไว้สูงๆ\n",
        "    4. \"frequency_penalty\": 0, ไปหาเอาเอง อธิบายไม่ถูก มีอีกหลาย param เดี๋ยวส่งแยกให้\n",
        "```\n",
        "\n",
        "> **gradio**: เป็นแค่ interface ที่เราใช้เทสโค้ดดูเดโม่เฉยๆ"
      ],
      "metadata": {
        "id": "OmA5DuxgCI-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the models and Qdrant client\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "qdrant_url = \"https://a63ffbf5-5568-46dd-9ec3-98751a51a998.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
        "api_key = \"S0QgrdtYHTC8f_53Nes2uJ4gWoxbPnIwkujhfRlwcWA_MOvuGseLXw\"\n",
        "collection_name = \"ELIXR\"\n",
        "client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
        "\n",
        "def get_image_vector(image):\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).convert(\"RGB\")\n",
        "    inputs = processor(images=pil_image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(**inputs)\n",
        "    return image_features.cpu().numpy().flatten()\n",
        "\n",
        "def find_top_k_similar_classes_with_qdrant(image, k=5):\n",
        "    image_vector = get_image_vector(image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=image_vector.tolist(),\n",
        "        limit=1000,\n",
        "        with_payload=True,\n",
        "        with_vectors=False\n",
        "    )\n",
        "    top_results = [result.payload.get(\"classname\", \"unknown\") for result in search_result]\n",
        "    unique_classes = list(dict.fromkeys(top_results))  # Keep only unique class names\n",
        "    filtered_top_k_classes = unique_classes[:k]\n",
        "\n",
        "    return filtered_top_k_classes\n",
        "\n",
        "# Function to interact with the LLM API\n",
        "def ask_llm(predicted_label, user_input, chat_history):\n",
        "    # Generate message using the newly created function\n",
        "    message = generate_message_prompt(predicted_label, user_input)\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": message}]\n",
        "\n",
        "    for item in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": item[0]})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": item[1]})\n",
        "\n",
        "    # Payload for the LLM API request\n",
        "    payload = {\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_tokens\": 100,\n",
        "        \"top_p\": 0.9,\n",
        "        \"frequency_penalty\": 0,\n",
        "        \"presence_penalty\": 0\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer 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\"  # Replace with your actual Bearer token\n",
        "    }\n",
        "\n",
        "    API_URL = \"https://ai-api.manageai.co.th/llm-model-03/v1/chat/completions\"\n",
        "\n",
        "    # Send the API request to the LLM\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        completion = response.json()\n",
        "        llm_response = completion['choices'][0]['message']['content']\n",
        "    else:\n",
        "        llm_response = f\"Error {response.status_code}: {response.text}\"\n",
        "\n",
        "    return llm_response\n",
        "\n",
        "\n",
        "def chat_interface(image, user_input, chat_history):\n",
        "    top_classes = find_top_k_similar_classes_with_qdrant(image, k=5)\n",
        "    top_prediction = top_classes[0] if top_classes else \"No prediction available.\"\n",
        "    llm_response = ask_llm(top_prediction, user_input, chat_history)\n",
        "\n",
        "    chat_history.append((user_input, llm_response))\n",
        "    return chat_history, chat_history\n",
        "\n",
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Lung Health Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            image_input = gr.Image(label=\"Upload Lung Image\", type=\"numpy\")\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(label=\"Chatbot\", height=400)\n",
        "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about lung health...\")\n",
        "            chat_state = gr.State([])  # Store chat history\n",
        "            submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    submit_button.click(fn=chat_interface,\n",
        "                        inputs=[image_input, user_input, chat_state],\n",
        "                        outputs=[chatbot, chat_state])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "ef9ZvopFCFDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **llm classification for answer simple question**\n",
        "\n",
        "อันนี้ สำหรับ task ให้มันตอบปัญหา (mmlu dataset)\n",
        "โดยเราให้คำถาม+choice (เป็นลิสต์) และให้มันตอบเป็นindex ของคำตอบที่ถูกต้อง"
      ],
      "metadata": {
        "id": "9UIbnoWyFucZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain_community huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkFNmDKzGdCC",
        "outputId": "6ec64ecd-366a-47d3-a581-c5d5e3e81928"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
        "from huggingface_hub import InferenceClient\n",
        "import concurrent.futures\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from langchain import PromptTemplate\n",
        "from threading import Semaphore"
      ],
      "metadata": {
        "id": "F6YFjlcbF0eD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmluprompt = \"\"\"\n",
        "Prompt: You are an expert in your field. Please read the question and choices provided below, understand the context, and carefully evaluate each choice. Your task is to select the index (0, 1, 2, or 3) of the correct answer. Respond with 'ANSWER INDEX: ' where  is your choice.\n",
        "\n",
        "Question: {question}\n",
        "Choices: {choices}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ywvrjluGGcO8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Example API URL and Key (replace with your actual API details)\n",
        "api_url = 'https://ai-api.manageai.co.th/llm-model-02/'\n",
        "api_key = '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'\n",
        "\n",
        "# Define the function to generate text based on the given prompt\n",
        "def fnmmlu(question, choices, mmluprompt, model_params, api_url, api_key):\n",
        "    formatted_prompt = mmluprompt.replace(\"{question}\", question).replace(\"{choices}\", str(choices))\n",
        "    client = InferenceClient(api_url, api_key)\n",
        "    response = client.text_generation(formatted_prompt, **model_params)\n",
        "    output = \"\".join(response)\n",
        "    return output\n",
        "\n",
        "def generate_answer(question, choices, mmluprompt, model_params, api_url, api_key):\n",
        "    result = fnmmlu(question, choices, mmluprompt, model_params, api_url, api_key)\n",
        "    return result\n",
        "\n",
        "# Define the model parameters\n",
        "model_params = {\n",
        "    \"max_new_tokens\": 10,\n",
        "    \"temperature\": 0.01,\n",
        "    \"top_p\": 0.95,\n",
        "    \"repetition_penalty\": 1.0\n",
        "}"
      ],
      "metadata": {
        "id": "Gxt92EjUGsDi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"What is the capital of France?\"\"\"\n",
        "choices = \"\"\"[\"New York\" \"Madrid\" \"Paris\" \"Rome\"]\"\"\"\n",
        "\n",
        "generated_answer = generate_answer(question, choices, mmluprompt, model_params, api_url, api_key)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u2QJcLRHMy5",
        "outputId": "43b4bfe1-d81a-4f4c-8c3e-402bf1b8b953"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in '__init__': pass token='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' as keyword args. From version 0.26 passing these as positional arguments will result in an error,\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER INDEX: 2\n",
            "```\n",
            "\n",
            "This\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"What is the capital of Thailand?\"\"\"\n",
        "choices = \"\"\"[\"Bangkok\" \"Madrid\" \"Paris\" \"Rome\"]\"\"\"\n",
        "\n",
        "generated_answer = generate_answer(question, choices, mmluprompt, model_params, api_url, api_key)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0ukOjffOIACs",
        "outputId": "bf0367db-1923-44b3-b417-fcf15df88444"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER INDEX: 0\n",
            "ANSWER INDEX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# จะเห็นว่าโมเดลมันก็ตอบถูกแหละ บางทีก็ halucinate หรือตอบใช้คำซ้ำๆๆๆมา เราก็แค่ต้องสร้างฟังกืชันมา extract คำตอบของมันเองอีกที\n",
        "# ส่วนใหญ่ classification task จะต้องสร้างฟังก์ชันจาก regular expression มาเอาแค่คำตอบที่เราอยากได้พอ\n",
        "# อย่างงานนี้เราก็แค่เขียนฟังก์ชันมาเก็บแค่ digit หลังคำว่า ANSWER INDEX"
      ],
      "metadata": {
        "id": "cR99ORpPIZOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJdHMhN5IVFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGoqJvXuITlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}