{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "base.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_huggingface langchain_community datasets"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "2l8KvSFKmdmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 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\n",
        "\n",
        "> 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\n",
        "\n",
        "> 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\n",
        "\n",
        "> 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"
      ],
      "metadata": {
        "id": "BE0QwMBcmdme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:56:55.747711Z",
          "iopub.execute_input": "2024-08-22T08:56:55.748194Z",
          "iopub.status.idle": "2024-08-22T08:56:57.795377Z",
          "shell.execute_reply.started": "2024-08-22T08:56:55.748146Z",
          "shell.execute_reply": "2024-08-22T08:56:57.794201Z"
        },
        "trusted": true,
        "id": "7A-qUDnpmdmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the MMLU dataset\n",
        "dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
        "test_df = dataset['test'].to_pandas()\n",
        "validation_df = dataset['validation'].to_pandas()\n",
        "dev_df = dataset['dev'].to_pandas()\n",
        "df = pd.concat([test_df, validation_df, dev_df], ignore_index=True)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T04:31:46.620075Z",
          "iopub.execute_input": "2024-08-22T04:31:46.620535Z",
          "iopub.status.idle": "2024-08-22T04:31:54.582945Z",
          "shell.execute_reply.started": "2024-08-22T04:31:46.620491Z",
          "shell.execute_reply": "2024-08-22T04:31:54.581623Z"
        },
        "trusted": true,
        "id": "l8SGiEvWmdml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
        "from huggingface_hub import InferenceClient"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T04:31:54.585225Z",
          "iopub.execute_input": "2024-08-22T04:31:54.58605Z",
          "iopub.status.idle": "2024-08-22T04:31:55.612094Z",
          "shell.execute_reply.started": "2024-08-22T04:31:54.585987Z",
          "shell.execute_reply": "2024-08-22T04:31:55.61097Z"
        },
        "trusted": true,
        "id": "df9d_LMQmdmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **function**"
      ],
      "metadata": {
        "id": "BjbVYER9mdmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_text_after_prompt_generated(text):\n",
        "    pattern = r\"PROMPT GENERATED:\\s*(.*)\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T04:44:08.327163Z",
          "iopub.execute_input": "2024-08-22T04:44:08.328226Z",
          "iopub.status.idle": "2024-08-22T04:44:08.333776Z",
          "shell.execute_reply.started": "2024-08-22T04:44:08.328177Z",
          "shell.execute_reply": "2024-08-22T04:44:08.332687Z"
        },
        "trusted": true,
        "id": "Ax_MSDJ-mdmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def extract_correct_answer(text):\n",
        "    text_upper = text.upper()\n",
        "    answer_index_pattern = (\n",
        "        r\"THE CORRECT ANSWER\\s+IS\\s*:\\s*(\\d+)\\s*$|\"   # 'The correct answer is: X'\n",
        "        r\"THE CORRECT ANSWER\\s+IS\\s*CHOICE\\s*(\\d+)\\s*,\\s*INDEX\\s*(\\d+)|\"  # 'The correct answer is choice X, index Y'\n",
        "        r\"ANSWER\\s*:\\s*<\\s*(\\d+)\\s*>|\"    # 'ANSWER: <number>'\n",
        "        r\"ANSWER\\s+INDEX\\s*:\\s*(\\d+)|\"    # 'ANSWER INDEX: number'\n",
        "        r\"ANSWER\\s*:\\s*(\\d+)|\"            # 'ANSWER: number'\n",
        "        r\"INDEX\\s*:\\s*(\\d+)|\"             # 'INDEX: number'\n",
        "        r\"THE CORRECT ANSWER\\s+IS\\s*:\\s*(\\d+)\\s*,|\"  # 'The correct answer is: X,'\n",
        "        r\"THE CORRECT ANSWER\\s+IS\\s*:\\s*(\\d+)\\s*.\"   # 'The correct answer is: X.'\n",
        "        r\"THE CORRECT ANSWER\\s+IS\\s*INDEX\\s*(\\d+)|\"  # 'The correct answer is index X.'\n",
        "        r\"DIRECT ANSWER\\s*:\\s*(\\d+)\"  # 'Direct Answer: <index>'\n",
        "    )\n",
        "\n",
        "    # Search for the answer index in the text\n",
        "    answer_index_match = re.search(answer_index_pattern, text_upper)\n",
        "\n",
        "    if answer_index_match:\n",
        "        # Print debug information to see which group matched\n",
        "#         print(f\"Matched groups: {answer_index_match.groups()}\")\n",
        "\n",
        "        # Return the first matched group that is not None\n",
        "        return int(answer_index_match.group(1) or\n",
        "                   answer_index_match.group(2) or\n",
        "                   answer_index_match.group(3) or\n",
        "                   answer_index_match.group(4) or\n",
        "                   answer_index_match.group(5) or\n",
        "                   answer_index_match.group(6) or\n",
        "                   answer_index_match.group(7) or\n",
        "                   answer_index_match.group(8))\n",
        "\n",
        "    return None\n",
        "\n",
        "def get_answer_index(answer, choices):\n",
        "    if isinstance(answer, str):  # If answer is a description\n",
        "        try:\n",
        "            return choices.index(answer.strip())\n",
        "        except ValueError:\n",
        "            return None\n",
        "    elif isinstance(answer, int):  # If answer is an index\n",
        "        return answer\n",
        "    return None\n",
        "\n",
        "def update_answer_index(row):\n",
        "    correct_answer = extract_correct_answer(row['prompt_answer'])\n",
        "\n",
        "    if correct_answer is not None:\n",
        "        # Ensure 'choices' is a list\n",
        "        choices_list = list(row['choices']) if isinstance(row['choices'], (pd.Series, np.ndarray)) else row['choices']\n",
        "        # Find the index of the correct answer in the choices list\n",
        "        answer_index = get_answer_index(correct_answer, choices_list)\n",
        "        return answer_index if answer_index is not None else \"\"\n",
        "\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:02:10.817556Z",
          "iopub.execute_input": "2024-08-22T10:02:10.818018Z",
          "iopub.status.idle": "2024-08-22T10:02:10.830969Z",
          "shell.execute_reply.started": "2024-08-22T10:02:10.817971Z",
          "shell.execute_reply": "2024-08-22T10:02:10.829529Z"
        },
        "trusted": true,
        "id": "z72NM_Smmdmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"\n",
        "    Extracts the answer index from the provided text using various patterns.\n",
        "    Returns the extracted number if found, otherwise returns None.\n",
        "    \"\"\"\n",
        "    # Patterns to match 'ANSWER:', 'ANSWER INDEX:', 'Answer: <number>', or 'index is <number>'\n",
        "    answer_pattern = r\"ANSWER:\\s*(\\d+)\"\n",
        "    answer_index_pattern = r\"ANSWER INDEX:\\s*(\\d+)\"\n",
        "    answer_in_angle_brackets_pattern = r\"Answer:\\s*<\\s*(\\d+)\\s*>\"\n",
        "    index_is_pattern = r\"index\\s+is\\s+(\\d+)\"\n",
        "\n",
        "    # Check for 'ANSWER:' pattern\n",
        "    answer_match = re.search(answer_pattern, text)\n",
        "    if answer_match:\n",
        "        return int(answer_match.group(1))\n",
        "\n",
        "    # Check for 'ANSWER INDEX:' pattern\n",
        "    answer_index_match = re.search(answer_index_pattern, text)\n",
        "    if answer_index_match:\n",
        "        return int(answer_index_match.group(1))\n",
        "\n",
        "    # Check for 'Answer: <number>' pattern\n",
        "    answer_in_angle_brackets_match = re.search(answer_in_angle_brackets_pattern, text)\n",
        "    if answer_in_angle_brackets_match:\n",
        "        return int(answer_in_angle_brackets_match.group(1))\n",
        "\n",
        "    # Check for 'index is <number>' pattern\n",
        "    index_is_match = re.search(index_is_pattern, text, re.IGNORECASE)\n",
        "    if index_is_match:\n",
        "        return int(index_is_match.group(1))\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T07:28:35.456488Z",
          "iopub.execute_input": "2024-08-22T07:28:35.457082Z",
          "iopub.status.idle": "2024-08-22T07:28:35.467463Z",
          "shell.execute_reply.started": "2024-08-22T07:28:35.457Z",
          "shell.execute_reply": "2024-08-22T07:28:35.46618Z"
        },
        "trusted": true,
        "id": "lFIFQ-85mdms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_relevant_output(text):\n",
        "    \"\"\"\n",
        "    Extracts and returns the portion of the text before specific keywords.\n",
        "    Keeps the content before any of the following keywords: \"Human:\", \"Assistant:\", \"System:\", \"###\", \"**\".\n",
        "    Args:\n",
        "        text (str): The output text to be processed.\n",
        "    Returns:\n",
        "        str: The extracted portion of the text.\n",
        "    \"\"\"\n",
        "    keywords = [\"Human:\", \"Assistant:\", \"System:\", \"###\", \"**\"]\n",
        "    pattern = r'(' + '|'.join(re.escape(keyword) for keyword in keywords) + r')'\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        return text[:match.start()].strip()\n",
        "    else:\n",
        "        return text.strip()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T06:28:00.30022Z",
          "iopub.status.idle": "2024-08-22T06:28:00.300769Z",
          "shell.execute_reply.started": "2024-08-22T06:28:00.300494Z",
          "shell.execute_reply": "2024-08-22T06:28:00.300515Z"
        },
        "trusted": true,
        "id": "ft9tFz3_mdmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BasePrompt**"
      ],
      "metadata": {
        "id": "8MOraexPmdmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from langchain import PromptTemplate\n",
        "\n",
        "# # Example API URL and Key (replace with your actual API details)\n",
        "# api_url = 'https://ai-api.manageai.co.th/llm-model-02/'\n",
        "# api_key = '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'\n",
        "\n",
        "# # Define the function to generate text based on the given prompt\n",
        "# def fn(instruction, prompt_template, model_params):\n",
        "#     # Format the base prompt with the given instruction\n",
        "#     formatted_prompt = prompt_template.replace(\"{instruction}\", instruction)\n",
        "\n",
        "#     # Ensure model_params is a dictionary\n",
        "#     if not isinstance(model_params, dict):\n",
        "#         raise ValueError(\"model_params must be a dictionary.\")\n",
        "\n",
        "#     # Generate text using the provided model parameters\n",
        "#     response = client.text_generation(formatted_prompt, **model_params)\n",
        "#     output = \"\".join(response)\n",
        "#     return output\n",
        "\n",
        "# def generate_prompt(instruction, api_url, api_key, prompt_template, model_params):\n",
        "#     # Define the model parameters\n",
        "#     default_params = {\n",
        "#         \"max_new_tokens\": 512,\n",
        "#         \"temperature\": 0.7,\n",
        "#         \"top_p\": 0.95,\n",
        "#         \"repetition_penalty\": 1.0\n",
        "#     }\n",
        "\n",
        "#     # Update default_params with any custom params provided\n",
        "#     model_params = {**default_params, **model_params}\n",
        "\n",
        "#     # Generate the prompt using the fn function\n",
        "#     result = fn(instruction, prompt_template, model_params)\n",
        "#     return result\n",
        "\n",
        "# # Define the model parameters\n",
        "# model_params = {\n",
        "#     \"max_new_tokens\": 512,\n",
        "#     \"temperature\": 0.7,\n",
        "#     \"top_p\": 0.95,\n",
        "#     \"repetition_penalty\": 1.0\n",
        "# }\n",
        "\n",
        "# client = InferenceClient(api_url, api_key)\n",
        "\n",
        "# # Example usage\n",
        "# custom_prompt = \"\"\"\n",
        "# YOU ARE AN EXPERT IN PROMPT ENGINEERING, CAPABLE OF CREATING HIGHLY EFFECTIVE AND PRECISE PROMPTS ACROSS ALL DISCIPLINES AND TASKS.\n",
        "# YOUR GOAL IS TO GENERATE A SPECIFIC PROMPT THAT GUIDES A LANGUAGE MODEL TO PERFORM THE TASK DESCRIBED BY THE USER IN THE MOST ACCURATE AND EFFICIENT WAY POSSIBLE.\n",
        "\n",
        "# ### TASK DESCRIPTION ###\n",
        "# - **USER INSTRUCTION**: {instruction}\n",
        "\n",
        "# ### PROMPT GENERATION GUIDELINES ###\n",
        "# 1. **IDENTIFY THE TASK TYPE**: Determine the nature of the task from the user's instruction. The task could involve translation, problem-solving, creative writing, code generation, data analysis, etc.\n",
        "#    - For **translation** tasks, ensure the prompt guides the translation accurately while maintaining the original tone and meaning.\n",
        "#    - For **problem-solving** tasks, guide the model to provide step-by-step solutions or hints without directly giving the answer.\n",
        "#    - For **code generation**, instruct the model to write or debug code based on the specific requirements given.\n",
        "#    - For **creative tasks**, like story writing or generating ideas, ensure the prompt fosters creativity while staying within the provided context.\n",
        "# 2. **LANGUAGE CONSISTENCY**: Ensure the generated prompt is in the same language as the user's instruction. If the task involves working in multiple languages, maintain the required language structure.\n",
        "# 3. **FOCUS AND CLARITY**: The prompt must be clear and focused on the specific task. Avoid including unrelated information or unnecessary complexity. Tailor the prompt to the exact needs of the task.\n",
        "# 4. **TASK-SPECIFIC ADAPTATION**: Adapt the prompt to be relevant to the domain of the task. For example:\n",
        "#    - **Scientific Analysis**: Emphasize precision and methodical steps.\n",
        "#    - **Mathematics**: Focus on logical progression and accurate calculations.\n",
        "#    - **Social Sciences**: Highlight critical thinking and context understanding.\n",
        "#    - **Arts and Humanities**: Encourage creativity and contextual interpretation.\n",
        "\n",
        "# ### WHAT NOT TO DO ###\n",
        "# - DO NOT PROVIDE DIRECT ANSWERS OR SOLUTIONS UNLESS SPECIFICALLY ASKED.\n",
        "# - DO NOT DEVIATE FROM THE TASK OR LANGUAGE SPECIFIED IN THE USER'S INSTRUCTION.\n",
        "# - AVOID INTRODUCING UNRELATED INFORMATION OR OVER-COMPLICATING THE PROMPT.\n",
        "# - DO NOT GENERATE PROMPTS THAT ARE TOO VAGUE, AMBIGUOUS, OR COMPLEX FOR THE TASK AT HAND.\n",
        "# - AVOID USING REPETITIVE OR UNNECESSARY LANGUAGE; KEEP THE PROMPT CLEAR AND FOCUSED.\n",
        "\n",
        "# ### THINKING PROCESS ###\n",
        "# 1. Analyze the user’s instruction to identify the task type and relevant field.\n",
        "# 2. Consider the language and context in which the task should be performed.\n",
        "# 3. Generate a prompt that is precise, clear, and aligned with the task's goals, ensuring it is tailored to the user’s requirements.\n",
        "# 4. Review the prompt to ensure it follows the guidelines, maintaining consistency and relevance to the task.\n",
        "\n",
        "# PROMPT GENERATED:\n",
        "# \"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T06:30:32.106602Z",
          "iopub.execute_input": "2024-08-22T06:30:32.107157Z",
          "iopub.status.idle": "2024-08-22T06:30:32.116508Z",
          "shell.execute_reply.started": "2024-08-22T06:30:32.107111Z",
          "shell.execute_reply": "2024-08-22T06:30:32.115285Z"
        },
        "trusted": true,
        "id": "eX7l5f0-mdmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **mixtral**"
      ],
      "metadata": {
        "id": "FHfo_Bf7mdmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "def setup_llm_chain(model_repo, temperature, top_p, max_new_tokens, repetition_penalty, custom_prompt):\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=model_repo,\n",
        "        model_kwargs={\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"repetition_penalty\": repetition_penalty\n",
        "        }\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=[\"instruction\"], template=custom_prompt)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    return chain\n",
        "\n",
        "def generate_prompt(instruction, custom_prompt, max_new_tokens):\n",
        "    # Model parameters\n",
        "    model_repo = \"mistralai/Mistral-7B-Instruct-v0.3\" # Ensure correct model repo ID\n",
        "    temperature = 0.7\n",
        "    top_p = 0.95\n",
        "    repetition_penalty = 1.0  # Adjusted for clarity\n",
        "\n",
        "    # Setup LLM chain\n",
        "    prompt_generator_chain = setup_llm_chain(model_repo, temperature, top_p, max_new_tokens, repetition_penalty, custom_prompt)\n",
        "\n",
        "    # Generate the output\n",
        "    result = prompt_generator_chain.run({\"instruction\": instruction})\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "custom_prompt = \"\"\"\n",
        "YOU ARE AN EXPERT IN PROMPT ENGINEERING, CAPABLE OF CREATING HIGHLY EFFECTIVE AND PRECISE PROMPTS ACROSS ALL DISCIPLINES AND TASKS.\n",
        "YOUR GOAL IS TO GENERATE A SPECIFIC PROMPT THAT GUIDES A LANGUAGE MODEL TO PERFORM THE TASK DESCRIBED BY THE USER IN THE MOST ACCURATE AND EFFICIENT WAY POSSIBLE.\n",
        "\n",
        "### TASK DESCRIPTION ###\n",
        "- **USER INSTRUCTION**: {instruction}\n",
        "\n",
        "### PROMPT GENERATION GUIDELINES ###\n",
        "1. **IDENTIFY THE TASK TYPE**: Determine the nature of the task from the user's instruction. The task could involve translation, problem-solving, creative writing, code generation, data analysis, etc.\n",
        "   - For **translation** tasks, ensure the prompt guides the translation accurately while maintaining the original tone and meaning.\n",
        "   - For **problem-solving** tasks, guide the model to provide step-by-step solutions or hints without directly giving the answer.\n",
        "   - For **code generation**, instruct the model to write or debug code based on the specific requirements given.\n",
        "   - For **creative tasks**, like story writing or generating ideas, ensure the prompt fosters creativity while staying within the provided context.\n",
        "2. **LANGUAGE CONSISTENCY**: Ensure the generated prompt is in the same language as the user's instruction. If the task involves working in multiple languages, maintain the required language structure.\n",
        "3. **FOCUS AND CLARITY**: The prompt must be clear and focused on the specific task. Avoid including unrelated information or unnecessary complexity. Tailor the prompt to the exact needs of the task.\n",
        "4. **TASK-SPECIFIC ADAPTATION**: Adapt the prompt to be relevant to the domain of the task. For example:\n",
        "   - **Scientific Analysis**: Emphasize precision and methodical steps.\n",
        "   - **Mathematics**: Focus on logical progression and accurate calculations.\n",
        "   - **Social Sciences**: Highlight critical thinking and context understanding.\n",
        "   - **Arts and Humanities**: Encourage creativity and contextual interpretation.\n",
        "\n",
        "### WHAT NOT TO DO ###\n",
        "- DO NOT PROVIDE DIRECT ANSWERS OR SOLUTIONS UNLESS SPECIFICALLY ASKED.\n",
        "- DO NOT DEVIATE FROM THE TASK OR LANGUAGE SPECIFIED IN THE USER'S INSTRUCTION.\n",
        "- AVOID INTRODUCING UNRELATED INFORMATION OR OVER-COMPLICATING THE PROMPT.\n",
        "- DO NOT GENERATE PROMPTS THAT ARE TOO VAGUE, AMBIGUOUS, OR COMPLEX FOR THE TASK AT HAND.\n",
        "- AVOID USING REPETITIVE OR UNNECESSARY LANGUAGE; KEEP THE PROMPT CLEAR AND FOCUSED.\n",
        "\n",
        "### THINKING PROCESS ###\n",
        "1. Analyze the user’s instruction to identify the task type and relevant field.\n",
        "2. Consider the language and context in which the task should be performed.\n",
        "3. Generate a prompt that is precise, clear, and aligned with the task's goals, ensuring it is tailored to the user’s requirements.\n",
        "4. Review the prompt to ensure it follows the guidelines, maintaining consistency and relevance to the task.\n",
        "\n",
        "PROMPT GENERATED:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T09:17:47.667131Z",
          "iopub.execute_input": "2024-08-22T09:17:47.667623Z",
          "iopub.status.idle": "2024-08-22T09:17:47.681164Z",
          "shell.execute_reply.started": "2024-08-22T09:17:47.667576Z",
          "shell.execute_reply": "2024-08-22T09:17:47.679861Z"
        },
        "trusted": true,
        "id": "XUSiunxfmdmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "instruction = \"\"\"แปลเพลงอังกฤษ \"twinkle little star\" เป็นภาษาไทย\"\"\"\n",
        "# generated_prompt = generate_prompt(instruction, api_url, api_key, custom_prompt, model_params)\n",
        "generated_prompt = generate_prompt(instruction, custom_prompt, max_new_tokens=50)\n",
        "extracted_text = extract_text_after_prompt_generated(generated_prompt)\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T09:17:49.877381Z",
          "iopub.execute_input": "2024-08-22T09:17:49.877847Z",
          "iopub.status.idle": "2024-08-22T09:17:49.985625Z",
          "shell.execute_reply.started": "2024-08-22T09:17:49.877784Z",
          "shell.execute_reply": "2024-08-22T09:17:49.984395Z"
        },
        "trusted": true,
        "id": "zc-qM_J3mdm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "instruction = \"\"\"\n",
        "You will be given a question and a set of choices from the MMLU dataset. Your task is to generate a precise response indicating the correct choice index based on comprehensive knowledge across all subject areas represented in the MMLU dataset.\n",
        "\n",
        "Question: {question}\n",
        "Choices: {choices}\n",
        "\n",
        "Instructions:\n",
        "1. Identify the Relevant Role: Based on the question's subject, assign yourself an appropriate expert role. Use the role that best fits the question to apply your expertise.\n",
        "2. Apply Expert Knowledge: Use your comprehensive knowledge in the identified subject area to evaluate each choice. Ensure that your reasoning and calculations reflect an expert understanding of the subject matter.\n",
        "3. Accurate Calculation and Correction: Perform necessary calculations or corrections specific to the subject area. Ensure precision and accuracy in your application of subject-specific knowledge.\n",
        "4. Consider All Choices: Evaluate all provided choices carefully within the context of the subject. Ensure that your final selection is the most accurate and appropriate based on your expertise and the information given.\n",
        "5. Direct Answer: Provide the correct choice index as a single integer in the format ANSWER INDEX: <index>, where <index> is one of the indices from 0 to 3. Do not include any explanations, justifications, or additional text.\n",
        "6. Strict Compliance: Ensure your response is directly one of the indices 0, 1, 2, or 3. Any number outside this range is invalid and should not be considered.\n",
        "7. Avoid Overthinking: Focus on providing a clear and accurate answer based on your expert knowledge and the provided information. Avoid unnecessary reasoning or interpretation.\n",
        "8. Error Elimination: Make sure your answer reflects correct knowledge and understanding of the principles involved in the subject. Ensure that all choices are considered before finalizing your response.\n",
        "\n",
        "Important:\n",
        "- The valid answer indices are strictly 0, 1, 2, or 3.\n",
        "- Any number outside this range is not a valid choice and should not be considered.\n",
        "- Make sure your response is directly one of these indices and match the indices. Remember the format of response.\n",
        "- Do not include any samples or description.\n",
        "\"\"\"\n",
        "# generated_prompt = generate_prompt(instruction, api_url, api_key, custom_prompt, model_params)\n",
        "generated_prompt = generate_prompt(instruction, custom_prompt, max_new_tokens=700)\n",
        "extracted_text = extract_text_after_prompt_generated(generated_prompt)\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:10:58.139913Z",
          "iopub.execute_input": "2024-08-22T10:10:58.140346Z",
          "iopub.status.idle": "2024-08-22T10:11:00.871372Z",
          "shell.execute_reply.started": "2024-08-22T10:10:58.140303Z",
          "shell.execute_reply": "2024-08-22T10:11:00.869979Z"
        },
        "trusted": true,
        "id": "i3CWCkCEmdm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **prompt for this task**"
      ],
      "metadata": {
        "id": "sgKBCvMHmdm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import time\n",
        "\n",
        "# Define the AI prompt with placeholders for question and choices\n",
        "mmluprompt = \"\"\"\n",
        "Question: {question}\n",
        "Choices: {choices}\n",
        "\n",
        "As an expert in problem-solving, carefully evaluate each choice to determine the most accurate answer. Focus on precision and clarity in your reasoning and calculations.\n",
        "\n",
        "Please provide the correct choice index as a single integer in the format ANSWER INDEX: <index>, where <index> is one of the indices from 0 to 3. Do not include any explanations, justifications, or additional text.\n",
        "\n",
        "Remember, the valid answer indices are strictly 0, 1, 2, or 3. Any number outside this range is not a valid choice and should not be considered.\n",
        "\n",
        "Important: Make sure your response is directly one of these indices and match the indices. Avoid any deviations or overthinking.\n",
        "\n",
        "ANSWER INDEX: <index>\n",
        "\"\"\"\n",
        "\n",
        "def setup_llm(model_repo, temperature, top_p, max_new_tokens, repetition_penalty, mmluprompt):\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=model_repo,\n",
        "        model_kwargs={\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"repetition_penalty\": repetition_penalty\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Update PromptTemplate to use 'question' and 'choices' as input variables\n",
        "    prompt = PromptTemplate(input_variables=[\"question\", \"choices\"], template=mmluprompt)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    return chain\n",
        "\n",
        "def mmlu_answer(question, choices, max_new_tokens=1500):\n",
        "    # Model parameters\n",
        "    model_repo = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Ensure correct model repo ID\n",
        "    temperature = 0.01\n",
        "    top_p = 0.95\n",
        "    repetition_penalty = 1.0  # Adjusted for clarity\n",
        "\n",
        "    # Setup LLM chain\n",
        "    prompt_generator_chain = setup_llm(model_repo, temperature, top_p, max_new_tokens, repetition_penalty, mmluprompt)\n",
        "\n",
        "    # Generate the output\n",
        "    result = prompt_generator_chain.run({\"question\": question, \"choices\": choices})\n",
        "    return result\n",
        "\n",
        "# Function to process each row for prompt_answer generation using the new MMLU prompt\n",
        "def generate_prompt_answer_optimized(row):\n",
        "    question = row.get('prompt_question', \"\")\n",
        "    choices = row.get('choices', \"\")  # Assuming 'choices' is a column in the DataFrame\n",
        "    response = mmlu_answer(question, choices)\n",
        "    return response.strip()\n",
        "\n",
        "# Function to process each batch\n",
        "def process_batch(batch_df, process_func):\n",
        "    return [process_func(row) for _, row in batch_df.iterrows()]\n",
        "\n",
        "# Function to process batches with multi-threading\n",
        "def batch_process(df, process_func, batch_size=32):\n",
        "    results = [None] * len(df)  # Initialize a list to store results in original order\n",
        "    num_batches = (len(df) + batch_size - 1) // batch_size  # Ceiling division\n",
        "    start_time = time.time()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = {}\n",
        "        for i in range(0, len(df), batch_size):\n",
        "            batch_df = df.iloc[i:i + batch_size]\n",
        "            future = executor.submit(process_batch, batch_df, process_func)\n",
        "            futures[future] = (i, i + batch_size)  # Store the index range for each future\n",
        "\n",
        "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
        "            start_idx, end_idx = futures[future]\n",
        "            batch_results = future.result()\n",
        "            results[start_idx:end_idx] = batch_results\n",
        "\n",
        "            # Calculate and print time per batch\n",
        "            elapsed_time = time.time() - start_time\n",
        "            completed_batches = (i + 1)\n",
        "            time_per_batch = elapsed_time / completed_batches\n",
        "            estimated_total_time = time_per_batch * num_batches\n",
        "            remaining_time = estimated_total_time - elapsed_time\n",
        "\n",
        "            print(f\"Processed batch {completed_batches}/{num_batches}\")\n",
        "            print(f\"Time for this batch: {elapsed_time:.2f} seconds\")\n",
        "            print(f\"Estimated total time: {estimated_total_time / 3600:.2f} hours\")\n",
        "            print(f\"Estimated remaining time: {remaining_time / 60:.2f} minutes\\n\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:11:14.157714Z",
          "iopub.execute_input": "2024-08-22T10:11:14.158184Z",
          "iopub.status.idle": "2024-08-22T10:11:14.176989Z",
          "shell.execute_reply.started": "2024-08-22T10:11:14.15814Z",
          "shell.execute_reply": "2024-08-22T10:11:14.175684Z"
        },
        "trusted": true,
        "id": "qJs2ARebmdm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"What is the capital of Thailand?\"\n",
        "choices = \"['Rome', 'Paris', 'Berlin', 'Bangkok']\"\n",
        "max_new_tokens = 100\n",
        "\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_answer(generated_answer)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:11:20.658772Z",
          "iopub.execute_input": "2024-08-22T10:11:20.659506Z",
          "iopub.status.idle": "2024-08-22T10:11:20.986189Z",
          "shell.execute_reply.started": "2024-08-22T10:11:20.659453Z",
          "shell.execute_reply": "2024-08-22T10:11:20.985092Z"
        },
        "trusted": true,
        "id": "JEJJkZRJmdm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"The ability to perceive your front door as a rectangle even when it is open, displaying a different retinal image is known as\"\"\"\n",
        "choices = \"\"\"['color constancy' 'closure' 'shape constancy' 'size constancy']\"\"\"\n",
        "\n",
        "max_new_tokens = 500\n",
        "\n",
        "# Get the answer from the model\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_answer(generated_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:08:15.918493Z",
          "iopub.execute_input": "2024-08-22T10:08:15.919283Z",
          "iopub.status.idle": "2024-08-22T10:08:18.366494Z",
          "shell.execute_reply.started": "2024-08-22T10:08:15.919235Z",
          "shell.execute_reply": "2024-08-22T10:08:18.365386Z"
        },
        "trusted": true,
        "id": "wJE2IhTumdm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive. \"\"\"\n",
        "choices = \"\"\"['0.0368' '0.473' '0.078' 'None of the above']\"\"\"\n",
        "\n",
        "max_new_tokens = 500\n",
        "# Get the answer from the model\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_answer(generated_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:12:43.508586Z",
          "iopub.execute_input": "2024-08-22T10:12:43.509048Z",
          "iopub.status.idle": "2024-08-22T10:12:43.612261Z",
          "shell.execute_reply.started": "2024-08-22T10:12:43.509004Z",
          "shell.execute_reply": "2024-08-22T10:12:43.611117Z"
        },
        "trusted": true,
        "id": "0TdFXdSmmdm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:12:46.074185Z",
          "iopub.execute_input": "2024-08-22T10:12:46.074616Z",
          "iopub.status.idle": "2024-08-22T10:12:46.08047Z",
          "shell.execute_reply.started": "2024-08-22T10:12:46.074573Z",
          "shell.execute_reply": "2024-08-22T10:12:46.079264Z"
        },
        "trusted": true,
        "id": "dQgxb7zPmdm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for \"tests positive.\" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive. \"\"\"\n",
        "choices = \"\"\"['0.0368' '0.473' '0.078' 'None of the above']\"\"\"\n",
        "\n",
        "max_new_tokens = 600\n",
        "\n",
        "# Get the answer from the model\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_correct_answer(generated_answer)\n",
        "answer_index = get_answer_index(answer, choices)\n",
        "print(answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:07:00.711432Z",
          "iopub.execute_input": "2024-08-22T10:07:00.711903Z",
          "iopub.status.idle": "2024-08-22T10:07:00.812025Z",
          "shell.execute_reply.started": "2024-08-22T10:07:00.711857Z",
          "shell.execute_reply": "2024-08-22T10:07:00.810785Z"
        },
        "trusted": true,
        "id": "bv7Rs80Mmdm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"The ability to perceive your front door as a rectangle even when it is open, displaying a different retinal image is known as\"\"\"\n",
        "choices = \"\"\"['color constancy' 'closure' 'shape constancy' 'size constancy']\"\"\"\n",
        "\n",
        "max_new_tokens = 1000\n",
        "\n",
        "# Get the answer from the model\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_correct_answer(generated_answer)\n",
        "answer_index = get_answer_index(answer, choices)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T10:03:18.94626Z",
          "iopub.execute_input": "2024-08-22T10:03:18.947389Z",
          "iopub.status.idle": "2024-08-22T10:03:19.855393Z",
          "shell.execute_reply.started": "2024-08-22T10:03:18.947338Z",
          "shell.execute_reply": "2024-08-22T10:03:19.854249Z"
        },
        "trusted": true,
        "id": "aBsZXa2cmdm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"Based on the paper “SoK: SSL and HTTPS: Revisiting past challenges and evaluating certificates trust model enhancements”, which of the following statements are false? \"\"\"\n",
        "choices = \"\"\"['Valid DV certificates provide more confidence to a user that she is connecting to the intended party than valid EV certificates.'\n",
        " 'OCSP stapling allows a server to prove to a browser that its certificate hasn’t been revoked.'\n",
        " 'DANE makes it difficult for an adversary to launch a SSL stripping attack.'\n",
        " 'Server key-pinning makes it harder for an adversary to convince a CA to mint a certificate for a site and launch an MITM attack on that site.'] \"\"\"\n",
        "\n",
        "max_new_tokens = 1500\n",
        "\n",
        "# Get the answer from the model\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_answer(generated_answer)\n",
        "print(answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T09:19:40.447921Z",
          "iopub.execute_input": "2024-08-22T09:19:40.448867Z",
          "iopub.status.idle": "2024-08-22T09:19:45.13586Z",
          "shell.execute_reply.started": "2024-08-22T09:19:40.448774Z",
          "shell.execute_reply": "2024-08-22T09:19:45.134592Z"
        },
        "trusted": true,
        "id": "WGMK8A3tmdm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "question = \"\"\"For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I was constantly taking time off of work because I was caring for my elderly parents. Scenario 2 | I engineered the bridge to fall down in the first stiff breeze.\"\"\"\n",
        "choices = \"\"\"[1:'Wrong, Wrong' 2:'Wrong, Not wrong' 3:'Not wrong, Wrong' 4:'Not wrong, Not wrong']\"\"\"\n",
        "\n",
        "max_new_tokens = 1500\n",
        "\n",
        "# Get the answer from the model\n",
        "generated_answer = mmlu_answer(question, choices, max_new_tokens)\n",
        "answer = extract_answer(generated_answer)\n",
        "print(generated_answer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T09:14:34.93742Z",
          "iopub.execute_input": "2024-08-22T09:14:34.937894Z",
          "iopub.status.idle": "2024-08-22T09:14:35.036443Z",
          "shell.execute_reply.started": "2024-08-22T09:14:34.93784Z",
          "shell.execute_reply": "2024-08-22T09:14:35.035301Z"
        },
        "trusted": true,
        "id": "PbgIl3S6mdnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['question'].iloc[15000], \"\\n\")\n",
        "print(df['choices'].iloc[15000], \"\\n\", df['answer'].iloc[15000])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T04:55:49.192092Z",
          "iopub.execute_input": "2024-08-22T04:55:49.192515Z",
          "iopub.status.idle": "2024-08-22T04:55:49.199315Z",
          "shell.execute_reply.started": "2024-08-22T04:55:49.192474Z",
          "shell.execute_reply": "2024-08-22T04:55:49.198046Z"
        },
        "trusted": true,
        "id": "BsRoUzcEmdnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# start"
      ],
      "metadata": {
        "id": "r7v1dLiwmdnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def sample_subject_rows(df, subjects, n=20):\n",
        "    \"\"\"\n",
        "    Sample 'n' rows per subject from the DataFrame for given subjects.\n",
        "    \"\"\"\n",
        "    # Filter DataFrame for specified subjects\n",
        "    filtered_df = df[df['subject'].isin(subjects)]\n",
        "\n",
        "    # Sample 'n' rows per subject\n",
        "    sampled_df_list = []\n",
        "    for subject in subjects:\n",
        "        subject_df = filtered_df[filtered_df['subject'] == subject]\n",
        "        sampled_df = subject_df.sample(n=min(n, len(subject_df)), random_state=42)\n",
        "        sampled_df_list.append(sampled_df)\n",
        "\n",
        "    return pd.concat(sampled_df_list).reset_index(drop=True)\n",
        "\n",
        "def run_async_processing(df, model_repo, temperature, top_p, max_new_tokens, repetition_penalty, mmluprompt, batch_size=32):\n",
        "    # Assuming run_async_processing is defined elsewhere and works as expected\n",
        "    return asyncio.run(generate_answers_for_batch_async(df, model_repo, temperature, top_p, max_new_tokens, repetition_penalty, mmluprompt, batch_size))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:55:51.480516Z",
          "iopub.execute_input": "2024-08-22T08:55:51.481115Z",
          "iopub.status.idle": "2024-08-22T08:55:51.49105Z",
          "shell.execute_reply.started": "2024-08-22T08:55:51.481051Z",
          "shell.execute_reply": "2024-08-22T08:55:51.489754Z"
        },
        "trusted": true,
        "id": "9FD7FywomdnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify subjects to sample from\n",
        "subjects = ['machine_learning', 'astronomy', 'nutrition', 'electrical_engineering']\n",
        "\n",
        "# Get sampled DataFrame\n",
        "sampled_df = sample_subject_rows(df, subjects, n=50)\n",
        "sampled_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:57:33.019523Z",
          "iopub.execute_input": "2024-08-22T08:57:33.019962Z",
          "iopub.status.idle": "2024-08-22T08:57:33.050622Z",
          "shell.execute_reply.started": "2024-08-22T08:57:33.01992Z",
          "shell.execute_reply": "2024-08-22T08:57:33.049149Z"
        },
        "trusted": true,
        "id": "cETXtpOumdnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df['prompt_answer'] = batch_process(sampled_df, generate_prompt_answer_optimized)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:57:38.147423Z",
          "iopub.execute_input": "2024-08-22T08:57:38.147895Z",
          "iopub.status.idle": "2024-08-22T08:58:15.771396Z",
          "shell.execute_reply.started": "2024-08-22T08:57:38.147845Z",
          "shell.execute_reply": "2024-08-22T08:58:15.770224Z"
        },
        "trusted": true,
        "id": "17RVM7C_mdnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "sampled_df['answer_index'] = sampled_df['prompt_answer'].apply(extract_answer)\n",
        "sampled_df['answer_index'] = sampled_df.apply(update_answer_index, axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:58:20.557741Z",
          "iopub.execute_input": "2024-08-22T08:58:20.558212Z",
          "iopub.status.idle": "2024-08-22T08:58:20.603251Z",
          "shell.execute_reply.started": "2024-08-22T08:58:20.558166Z",
          "shell.execute_reply": "2024-08-22T08:58:20.602206Z"
        },
        "trusted": true,
        "id": "d9p1j9WRmdnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df['answer_index'].isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:58:31.957932Z",
          "iopub.execute_input": "2024-08-22T08:58:31.958393Z",
          "iopub.status.idle": "2024-08-22T08:58:31.967622Z",
          "shell.execute_reply.started": "2024-08-22T08:58:31.958346Z",
          "shell.execute_reply": "2024-08-22T08:58:31.966052Z"
        },
        "trusted": true,
        "id": "WAhmKg8fmdnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_exact_match(predictions, references):\n",
        "    correct = 0\n",
        "    total = len(predictions)\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        if pred == ref:\n",
        "            correct += 1\n",
        "    return correct / total * 100 if total > 0 else 0\n",
        "\n",
        "def compute_exact_match_per_subject(df):\n",
        "    \"\"\"\n",
        "    Computes the exact match score for each subject in the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the columns 'subject', 'answer_index', and 'answer'.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series where the index is the subject and the value is the exact match score.\n",
        "    \"\"\"\n",
        "    # Group by subject and compute exact match for each group\n",
        "    exact_match_scores = df.groupby('subject').apply(\n",
        "        lambda group: compute_exact_match(group['answer_index'].tolist(), group['answer'].tolist())\n",
        "    )\n",
        "    return exact_match_scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:58:39.937725Z",
          "iopub.execute_input": "2024-08-22T08:58:39.938838Z",
          "iopub.status.idle": "2024-08-22T08:58:39.947304Z",
          "shell.execute_reply.started": "2024-08-22T08:58:39.938741Z",
          "shell.execute_reply": "2024-08-22T08:58:39.946017Z"
        },
        "trusted": true,
        "id": "Rk_pk0ROmdnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute exact match per subject\n",
        "exact_match_per_subject = compute_exact_match_per_subject(sampled_df)\n",
        "print(exact_match_per_subject)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:58:46.020454Z",
          "iopub.execute_input": "2024-08-22T08:58:46.020948Z",
          "iopub.status.idle": "2024-08-22T08:58:46.031977Z",
          "shell.execute_reply.started": "2024-08-22T08:58:46.020899Z",
          "shell.execute_reply": "2024-08-22T08:58:46.030689Z"
        },
        "trusted": true,
        "id": "4Vh_d2ZlmdnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df[sampled_df['answer'] != sampled_df['answer_index']]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T08:58:56.554115Z",
          "iopub.execute_input": "2024-08-22T08:58:56.55456Z",
          "iopub.status.idle": "2024-08-22T08:58:56.576596Z",
          "shell.execute_reply.started": "2024-08-22T08:58:56.554517Z",
          "shell.execute_reply": "2024-08-22T08:58:56.575518Z"
        },
        "trusted": true,
        "id": "REbFka63mdnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampled_df[sampled_df['answer'] != sampled_df['answer_index']]['question'].iloc[1], \"\\n\")\n",
        "print(sampled_df[sampled_df['answer'] != sampled_df['answer_index']]['choices'].iloc[1], \"\\n\")\n",
        "print(sampled_df[sampled_df['answer'] != sampled_df['answer_index']]['answer'].iloc[1], \"\\n\")\n",
        "print(sampled_df[sampled_df['answer'] != sampled_df['answer_index']]['prompt_answer'].iloc[1], \"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-22T09:20:11.363356Z",
          "iopub.execute_input": "2024-08-22T09:20:11.363839Z",
          "iopub.status.idle": "2024-08-22T09:20:11.375724Z",
          "shell.execute_reply.started": "2024-08-22T09:20:11.36378Z",
          "shell.execute_reply": "2024-08-22T09:20:11.374638Z"
        },
        "trusted": true,
        "id": "p9SzUdO5mdnH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}