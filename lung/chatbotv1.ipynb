{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZDfexX9RcrvKr6BO04y5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nxxk23/AI-Engineer/blob/main/lung/chatbotv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G8W1pwp-eIKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WlhgV1QeFMw",
        "outputId": "f04981bb-2da9-41c7-ec78-35ad043b92b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image\n",
        "url = \"/content/drive/MyDrive/AIEngineer/lung/infection lung.jpg\"\n",
        "image = Image.open(url)\n",
        "\n",
        "# Assuming processor and model are already defined\n",
        "inputs = processor(text=[\"normal lung\", \"infection lung\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "print(probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwGA3JODevCL",
        "outputId": "7ab4f147-9357-466a-f9f8-b09b0eb6a0ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3086, 0.6914]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your text labels\n",
        "labels = [\"normal lung\", \"infection lung\"]\n",
        "\n",
        "# Get the index of the label with the highest probability\n",
        "predicted_index = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "# Output the corresponding label\n",
        "predicted_label = labels[predicted_index]\n",
        "\n",
        "print(f\"Predicted label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie4aRETHe_wm",
        "outputId": "316bcfd0-8c9e-4e41-f1cc-be6586e72662"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: infection lung\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your text labels\n",
        "labels = [\"normal lung\", \"infection lung\"]\n",
        "\n",
        "# Iterate over the probabilities and labels to print them\n",
        "for i, prob in enumerate(probs[0]):\n",
        "    print(f\"There is a {prob.item() * 100:.2f}% chance that the image represents a '{labels[i]}.'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp7lo40YgHqX",
        "outputId": "884817f9-e1fa-4dda-e651-8f4e78600b8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is a 30.86% chance that the image represents a 'normal lung.'\n",
            "There is a 69.14% chance that the image represents a 'infection lung.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch requests gradio"
      ],
      "metadata": {
        "id": "iXs9MN9jhjEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when I use this why i can ask only 1 question it should be like chat i can interact\n",
        "import torch\n",
        "import requests\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Function to classify the image using CLIP\n",
        "def classify_image(image):\n",
        "    # Define the text descriptions\n",
        "    texts = [\"normal lung\", \"infection lung\"]\n",
        "\n",
        "    # Process the image and texts for CLIP\n",
        "    inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    # Get the logits for image-text similarity\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "\n",
        "    # Get probabilities\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "    # Determine the label with the highest probability\n",
        "    predicted_index = torch.argmax(probs, dim=1).item()\n",
        "    predicted_label = texts[predicted_index]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Function to interact with the LLM API\n",
        "def ask_llm(image, user_input, chat_history):\n",
        "    # Classify the image using CLIP\n",
        "    predicted_label = classify_image(image)\n",
        "\n",
        "    # Prepare the LLM messages (including chat history + new user question)\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a medical assistant with expertise in diagnosing lung conditions from medical images.\"}]\n",
        "\n",
        "    # Add previous chat history to the messages\n",
        "    for item in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": item[\"user\"]})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": item[\"bot\"]})\n",
        "\n",
        "    # Add current image classification + user input to the LLM\n",
        "    # Explicitly state that the image has been analyzed\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"The image has been analyzed and classified as showing '{predicted_label}'. {user_input}\"})\n",
        "\n",
        "    # Payload for the LLM API request\n",
        "    payload = {\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_tokens\": 100,\n",
        "        \"top_p\": 0.9,\n",
        "        \"frequency_penalty\": 0,\n",
        "        \"presence_penalty\": 0\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer bWFuYWdlYWlfdXNlcjpTZTNyZXQjMTIz\"  # Replace with actual Bearer token\n",
        "    }\n",
        "\n",
        "    API_URL = \"https://ai-api.manageai.co.th/llm-model-03/v1/chat/completions\"\n",
        "\n",
        "    # Send the API request to the LLM\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        completion = response.json()\n",
        "        llm_response = completion['choices'][0]['message']['content']\n",
        "    else:\n",
        "        llm_response = f\"Error {response.status_code}: {response.text}\"\n",
        "\n",
        "    # Append the latest conversation to chat history\n",
        "    chat_history.append((user_input, llm_response))\n",
        "\n",
        "    return chat_history, chat_history\n",
        "\n",
        "# Gradio interface function\n",
        "def chat_interface(image, user_input, chat_history=[]):\n",
        "    # Process the image and user input through the CLIP and LLM models\n",
        "    chat_history, updated_history = ask_llm(image, user_input, chat_history)\n",
        "\n",
        "    return updated_history, updated_history\n",
        "\n",
        "# Define the Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Lung Health Chatbot\")\n",
        "\n",
        "    with gr.Row():  # Create a row to hold the image and chatbot\n",
        "        with gr.Column(scale=1):  # Left column for the image (smaller scale)\n",
        "            image_input = gr.Image(label=\"Upload Lung Image\", type=\"pil\")\n",
        "\n",
        "        with gr.Column(scale=2):  # Right column for the chatbot (larger scale)\n",
        "            chatbot = gr.Chatbot(label=\"Chatbot\", height=400)\n",
        "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about lung health...\")\n",
        "            chat_state = gr.State([])  # Store chat history\n",
        "            submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    submit_button.click(fn=chat_interface,\n",
        "                        inputs=[image_input, user_input, chat_state],\n",
        "                        outputs=[chatbot, chat_state])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "B9HrJORro9nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 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"
      ],
      "metadata": {
        "id": "Rv_pCS4r4mLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Function to classify the image using CLIP\n",
        "def classify_image(image):\n",
        "    # Define the text descriptions\n",
        "    texts = [\"normal lung\", \"infection lung\"]\n",
        "    inputs = clip_processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    predicted_index = torch.argmax(probs, dim=1).item()\n",
        "    predicted_label = texts[predicted_index]\n",
        "    return predicted_label\n",
        "\n",
        "def ask_llm(image, user_input, chat_history):\n",
        "    predicted_label = classify_image(image)\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a medical assistant with expertise in diagnosing lung conditions from medical images.\"}]\n",
        "    for item in chat_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": item[0]})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": item[1]})\n",
        "\n",
        "    # Add current image classification + user input to the LLM\n",
        "    messages.append({\"role\": \"user\", \"content\": f\"The image has been analyzed and classified as showing '{predicted_label}'. {user_input}\"})\n",
        "    payload = {\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_tokens\": 100,\n",
        "        \"top_p\": 0.9,\n",
        "        \"frequency_penalty\": 0,\n",
        "        \"presence_penalty\": 0\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer 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\"  # Replace with actual Bearer token\n",
        "    }\n",
        "\n",
        "    API_URL = \"https://ai-api.manageai.co.th/llm-model-03/v1/chat/completions\"\n",
        "\n",
        "    # Send the API request to the LLM\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        completion = response.json()\n",
        "        llm_response = completion['choices'][0]['message']['content']\n",
        "    else:\n",
        "        llm_response = f\"Error {response.status_code}: {response.text}\"\n",
        "\n",
        "    # Append the latest conversation to chat history\n",
        "    chat_history.append((user_input, llm_response))\n",
        "\n",
        "    return chat_history\n",
        "\n",
        "# Gradio interface function\n",
        "def chat_interface(image, user_input, chat_history):\n",
        "    # Process the image and user input through the CLIP and LLM models\n",
        "    updated_history = ask_llm(image, user_input, chat_history)\n",
        "    return updated_history, updated_history\n",
        "\n",
        "\n",
        "# Define the Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Lung Health Chatbot\")\n",
        "\n",
        "    with gr.Row():  # Create a row to hold the image and chatbot\n",
        "        with gr.Column(scale=1):  # Left column for the image (smaller scale)\n",
        "            image_input = gr.Image(label=\"Upload Lung Image\", type=\"pil\")\n",
        "\n",
        "        with gr.Column(scale=2):  # Right column for the chatbot (larger scale)\n",
        "            chatbot = gr.Chatbot(label=\"Chatbot\", height=400)\n",
        "            user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Ask about lung health...\")\n",
        "            chat_state = gr.State([])  # Store chat history\n",
        "            submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    submit_button.click(fn=chat_interface,\n",
        "                        inputs=[image_input, user_input, chat_state],\n",
        "                        outputs=[chatbot, chat_state])\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "cRnUDkgxiRNI",
        "outputId": "6ff3d1e5-647a-4053-9b99-8e78952d0794"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a6068efc389647a967.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6068efc389647a967.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7865 <> https://a6068efc389647a967.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1gg3mHceoiu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}